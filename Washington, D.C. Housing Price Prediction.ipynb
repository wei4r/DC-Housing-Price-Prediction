{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (before encoding): (109034, 39)\n",
      "Data Encoded Shape: (56567, 176)\n",
      "Augmented Data Shape: (105890, 176)\n",
      "Final Data Shape: (162457, 176)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>LANDAREA</th>\n",
       "      <th>Building_Density</th>\n",
       "      <th>Bedroom_Room_Ratio</th>\n",
       "      <th>Sale_Year</th>\n",
       "      <th>Sale_Month</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>162319.000000</td>\n",
       "      <td>162457.000000</td>\n",
       "      <td>162457.000000</td>\n",
       "      <td>162457.000000</td>\n",
       "      <td>162457.000000</td>\n",
       "      <td>162457.000000</td>\n",
       "      <td>158835.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.695291</td>\n",
       "      <td>3382.282943</td>\n",
       "      <td>0.789196</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>2009.942532</td>\n",
       "      <td>6.383172</td>\n",
       "      <td>0.966796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.402255</td>\n",
       "      <td>6489.465822</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.142796</td>\n",
       "      <td>24.379180</td>\n",
       "      <td>3.437669</td>\n",
       "      <td>0.948020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1520.000000</td>\n",
       "      <td>0.421710</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2217.000000</td>\n",
       "      <td>0.703422</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>4263.000000</td>\n",
       "      <td>1.059050</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>942632.000000</td>\n",
       "      <td>4.988943</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ROOMS       LANDAREA  Building_Density  Bedroom_Room_Ratio  \\\n",
       "count  162319.000000  162457.000000     162457.000000       162457.000000   \n",
       "mean        7.695291    3382.282943          0.789196            0.480144   \n",
       "std         2.402255    6489.465822          0.461382            0.142796   \n",
       "min         1.000000     216.000000          0.017211            0.000000   \n",
       "25%         6.000000    1520.000000          0.421710            0.400000   \n",
       "50%         7.000000    2217.000000          0.703422            0.500000   \n",
       "75%         9.000000    4263.000000          1.059050            0.500000   \n",
       "max        48.000000  942632.000000          4.988943            8.000000   \n",
       "\n",
       "           Sale_Year     Sale_Month        Cluster  \n",
       "count  162457.000000  162457.000000  158835.000000  \n",
       "mean     2009.942532       6.383172       0.966796  \n",
       "std        24.379180       3.437669       0.948020  \n",
       "min      1900.000000       1.000000       0.000000  \n",
       "25%      2009.000000       4.000000       0.000000  \n",
       "50%      2017.000000       6.000000       1.000000  \n",
       "75%      2021.000000       9.000000       2.000000  \n",
       "max      2024.000000      12.000000       4.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Computer_Assisted_Mass_Appraisal_-_Residential.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical columns for One-Hot Encoding\n",
    "categorical_features = ['HEAT', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'USECODE']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "onehot_encoder = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=int), categorical_features)], remainder='passthrough')\n",
    "data_encoded = onehot_encoder.fit_transform(data)\n",
    "\n",
    "# Update column names for one-hot encoded features\n",
    "encoded_feature_names = onehot_encoder.named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "data_encoded_df = pd.DataFrame(data_encoded, columns=list(encoded_feature_names) + list(data.columns.drop(categorical_features)))\n",
    "\n",
    "# Handle zero values in certain columns to prevent division errors in interaction features\n",
    "data_encoded_df = data_encoded_df.replace({'LANDAREA': {0: np.nan}, 'ROOMS': {0: np.nan}})\n",
    "\n",
    "# Create interaction features\n",
    "data_encoded_df['Rooms_Bathrooms'] = data_encoded_df['ROOMS'] * data_encoded_df['BATHRM']\n",
    "data_encoded_df['Building_Density'] = data_encoded_df['GBA'] / data_encoded_df['LANDAREA']\n",
    "data_encoded_df['Bedroom_Room_Ratio'] = data_encoded_df['BEDRM'] / data_encoded_df['ROOMS']\n",
    "\n",
    "# Fill NaN values after division\n",
    "data_encoded_df = data_encoded_df.fillna({'Building_Density': 0, 'Bedroom_Room_Ratio': 0})\n",
    "\n",
    "# Handle invalid values\n",
    "data_encoded_df = data_encoded_df[\n",
    "    (data_encoded_df['YR_RMDL'] >= 1000) & (data_encoded_df['YR_RMDL'] <= 2023) &\n",
    "    (data_encoded_df['AYB'] >= 1000) & (data_encoded_df['AYB'] <= 2023) &\n",
    "    (data_encoded_df['EYB'] >= 1000) & (data_encoded_df['EYB'] <= 2023) &\n",
    "    (data_encoded_df['GBA'] > 0) & (data_encoded_df['LANDAREA'] > 0) \n",
    "]\n",
    "\n",
    "\n",
    "# Convert SALEDATE to datetime format and derive date-based features\n",
    "data_encoded_df['SALEDATE'] = pd.to_datetime(data_encoded_df['SALEDATE'])\n",
    "data_encoded_df['Property_Age'] = 2023 - data_encoded_df['AYB']\n",
    "data_encoded_df['Years_Since_Remodel'] = 2023 - data_encoded_df['YR_RMDL']\n",
    "data_encoded_df['Years_Between_Built_and_Remodel'] = data_encoded_df['YR_RMDL'] - data_encoded_df['AYB']\n",
    "data_encoded_df['Sale_Year'] = data_encoded_df['SALEDATE'].dt.year\n",
    "data_encoded_df['Sale_Month'] = data_encoded_df['SALEDATE'].dt.month\n",
    "\n",
    "# Define features for clustering (using numeric features only for simplicity)\n",
    "features_for_clustering = ['ROOMS', 'BATHRM', 'LANDAREA', 'GBA', 'PRICE']\n",
    "data_cluster = data_encoded_df[features_for_clustering].dropna()\n",
    "\n",
    "# Preserve original index for later merge\n",
    "data_cluster = data_cluster.reset_index()  # This adds the original index as a column\n",
    "\n",
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cluster[features_for_clustering])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "n_clusters = 5  # Set an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "data_cluster['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Merge cluster labels back to original data\n",
    "data_encoded_df = data_encoded_df.merge(data_cluster[['index', 'Cluster']], left_index=True, right_on='index', how='left')\n",
    "data_encoded_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Remove rows with NaN in Cluster column for SMOTE application\n",
    "data_with_clusters = data_encoded_df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Placeholder for augmented data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Apply random oversampling within each cluster\n",
    "for cluster in data_with_clusters['Cluster'].unique():\n",
    "    # Select data for the current cluster\n",
    "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "    \n",
    "    # Set the desired size for oversampling (e.g., double the original size of each cluster)\n",
    "    target_size = len(cluster_data) * 2\n",
    "    \n",
    "    # Perform random oversampling\n",
    "    cluster_augmented = resample(cluster_data, replace=True, n_samples=target_size, random_state=42)\n",
    "    \n",
    "    # Append to augmented data\n",
    "    augmented_data = pd.concat([augmented_data, cluster_augmented])\n",
    "\n",
    "# Combine original data with augmented data\n",
    "final_data = pd.concat([data_encoded_df, augmented_data], ignore_index=True)\n",
    "\n",
    "# Check final dataset shape and display a sample\n",
    "print(\"Original Data Shape (before encoding):\", data.shape)\n",
    "print(\"Data Encoded Shape:\", data_encoded_df.shape)\n",
    "print(\"Augmented Data Shape:\", augmented_data.shape)\n",
    "print(\"Final Data Shape:\", final_data.shape)\n",
    "final_data.head()\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after processing: 0\n",
      "Training set shape: (129965, 162)\n",
      "Test set shape: (32492, 162)\n"
     ]
    }
   ],
   "source": [
    "data = final_data.copy()\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Fill missing values in numeric columns with the median\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check that missing values have been handled\n",
    "print(\"Missing values after processing:\", data.isnull().sum().sum())\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "# Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Step 3: Detect and handle outliers\n",
    "# # Use Z-score method to detect outliers; replace values with NaN if Z-score > 3\n",
    "# z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "# data[numeric_columns] = np.where(z_scores > 3, np.nan, data[numeric_columns])\n",
    "# # Fill the NaN values resulting from outliers with the median\n",
    "# data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "# Assuming 'PRICE' is the target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "\n",
    "# Ensure that only numeric columns are included in X for model training\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理有問題的數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric Features Summary (including Q1, Q2, Q3, IQR):\n",
      "                        count          mean       std       min       25%  \\\n",
      "ROOMS               162457.0  3.918860e-17  1.000003 -2.787934 -0.705739   \n",
      "LANDAREA            162457.0 -2.099389e-17  1.000003 -0.487913 -0.286971   \n",
      "Building_Density    162457.0 -3.359023e-17  1.000003 -1.673208 -0.796493   \n",
      "Bedroom_Room_Ratio  162457.0 -6.613077e-17  1.000003 -3.362458 -0.561252   \n",
      "Sale_Year           162457.0  8.908409e-16  1.000003 -4.509703 -0.038661   \n",
      "Sale_Month          162457.0  6.210694e-17  1.000003 -1.565942 -0.693254   \n",
      "Cluster             162457.0  1.119674e-17  1.000003 -1.032146 -1.032146   \n",
      "\n",
      "                         50%       75%         max       IQR  \n",
      "ROOMS              -0.289300  0.543578   16.784698  1.249317  \n",
      "LANDAREA           -0.179566  0.135715  144.734965  0.422686  \n",
      "Building_Density   -0.185907  0.584884    9.102572  1.381378  \n",
      "Bedroom_Room_Ratio  0.139049  0.139049   52.661645  0.700301  \n",
      "Sale_Year           0.289488  0.453563    0.576620  0.492225  \n",
      "Sale_Month         -0.111463  0.761224    1.633911  1.454478  \n",
      "Cluster             0.034632  1.101409    3.234965  2.133555  \n",
      "\n",
      "Percentage of Outliers in Each Numeric Feature:\n",
      "                     Outliers (%)\n",
      "ROOMS                   2.067624\n",
      "LANDAREA                0.347784\n",
      "Building_Density        1.138763\n",
      "Bedroom_Room_Ratio      0.712804\n",
      "Sale_Year               4.263898\n",
      "Sale_Month              0.000000\n",
      "Cluster                 0.003693\n"
     ]
    }
   ],
   "source": [
    "# 檢查每個特徵的數據類型\n",
    "# print(\"Data Types for Each Feature:\\n\", data.dtypes)\n",
    "\n",
    "# 計算每個數值特徵的描述統計量（包括 Q1, Q2 (中位數), Q3）\n",
    "numeric_data = data.drop(columns=categorical_columns)\n",
    "numeric_summary = numeric_data.describe(percentiles=[0.25, 0.5, 0.75]).T  # 使用轉置方便閱讀\n",
    "numeric_summary['IQR'] = numeric_summary['75%'] - numeric_summary['25%']  # 計算 IQR\n",
    "\n",
    "# 顯示每個特徵的四分位數信息\n",
    "print(\"\\nNumeric Features Summary (including Q1, Q2, Q3, IQR):\\n\", numeric_summary)\n",
    "\n",
    "# 檢查極端值（異常值）百分比\n",
    "# 計算 Z-score 超過閾值的異常值比例\n",
    "outliers_percentage = {}\n",
    "\n",
    "for col in numeric_data.columns:\n",
    "    z_scores = stats.zscore(data[col].dropna())  # 排除缺失值，計算 Z-score\n",
    "    outliers_percentage[col] = (np.abs(z_scores) > 3).mean() * 100  # 計算 Z-score > 3 的異常值比例\n",
    "\n",
    "outliers_percentage_df = pd.DataFrame.from_dict(outliers_percentage, orient='index', columns=['Outliers (%)'])\n",
    "print(\"\\nPercentage of Outliers in Each Numeric Feature:\\n\", outliers_percentage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAT_0.0 unique values: [0 1]\n",
      "HEAT_1.0 unique values: [0 1]\n",
      "HEAT_2.0 unique values: [0 1]\n",
      "HEAT_3.0 unique values: [0 1]\n",
      "HEAT_4.0 unique values: [0 1]\n",
      "HEAT_5.0 unique values: [0 1]\n",
      "HEAT_6.0 unique values: [0 1]\n",
      "HEAT_7.0 unique values: [0 1]\n",
      "HEAT_8.0 unique values: [1 0]\n",
      "HEAT_9.0 unique values: [0 1]\n",
      "HEAT_10.0 unique values: [0 1]\n",
      "HEAT_11.0 unique values: [0 1]\n",
      "HEAT_12.0 unique values: [0 1]\n",
      "HEAT_13.0 unique values: [0 1]\n",
      "HEAT_nan unique values: [0 1]\n",
      "STYLE_0.0 unique values: [0 1]\n",
      "STYLE_1.0 unique values: [0 1]\n",
      "STYLE_2.0 unique values: [0 1]\n",
      "STYLE_3.0 unique values: [0 1]\n",
      "STYLE_4.0 unique values: [0 1]\n",
      "STYLE_5.0 unique values: [0 1]\n",
      "STYLE_6.0 unique values: [0 1]\n",
      "STYLE_7.0 unique values: [0 1]\n",
      "STYLE_8.0 unique values: [0 1]\n",
      "STYLE_9.0 unique values: [0 1]\n",
      "STYLE_10.0 unique values: [1 0]\n",
      "STYLE_11.0 unique values: [0 1]\n",
      "STYLE_12.0 unique values: [0 1]\n",
      "STYLE_13.0 unique values: [0 1]\n",
      "STYLE_14.0 unique values: [0 1]\n",
      "STYLE_15.0 unique values: [0 1]\n",
      "STYLE_94.0 unique values: [0]\n",
      "STYLE_99.0 unique values: [0 1]\n",
      "STYLE_nan unique values: [0 1]\n",
      "STRUCT_0.0 unique values: [0 1]\n",
      "STRUCT_1.0 unique values: [0 1]\n",
      "STRUCT_2.0 unique values: [0 1]\n",
      "STRUCT_4.0 unique values: [0 1]\n",
      "STRUCT_5.0 unique values: [0 1]\n",
      "STRUCT_6.0 unique values: [1 0]\n",
      "STRUCT_7.0 unique values: [0 1]\n",
      "STRUCT_8.0 unique values: [0 1]\n",
      "STRUCT_13.0 unique values: [0]\n",
      "STRUCT_nan unique values: [0 1]\n",
      "HEAT_D unique values: ['Ht Pump' 'Forced Air' 'Warm Cool' 'Hot Water Rad' 'Water Base Brd'\n",
      " 'Elec Base Brd' 'Wall Furnace' 'Electric Rad' 'Air Exchng' 'Evp Cool'\n",
      " 'Gravity Furnac' 'Ind Unit' 'Air-Oil' 'No Data']\n",
      "STYLE_D unique values: ['4 Story' '3 Story' '2.5 Story Fin' '1.5 Story Fin' '3.5 Story Fin'\n",
      " '2 Story' '3.5 Story Unfin' '2.5 Story Unfin' '1 Story' 'No Data'\n",
      " '4.5 Story Unfin' '1.5 Story Unfin' 'Bi-Level' 'Split Level'\n",
      " '4.5 Story Fin' 'Split Foyer' 'Vacant']\n",
      "STRUCT_D unique values: ['Row End' 'Row Inside' 'Single' 'Multi' 'Semi-Detached' 'Town Inside'\n",
      " 'Town End' 'No Data']\n"
     ]
    }
   ],
   "source": [
    "# 檢查 One-Hot Encoding 特徵的唯一值\n",
    "one_hot_columns = [col for col in data.columns if 'HEAT_' in col or 'STYLE_' in col or 'STRUCT_' in col]  # 替換為您使用 One-Hot Encoding 的特徵前綴\n",
    "for col in one_hot_columns:\n",
    "    print(f\"{col} unique values:\", data[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years_Since_Remodel'] = data['Years_Since_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "data['Years_Between_Built_and_Remodel'] = data['Years_Between_Built_and_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "# 用中位數填補 NaN\n",
    "data['Years_Since_Remodel'].fillna(data['Years_Since_Remodel'].median(), inplace=True)\n",
    "data['Years_Between_Built_and_Remodel'].fillna(data['Years_Between_Built_and_Remodel'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Sale_Month: [ 0.47032823  0.17943255  1.05211959 -0.69325449  1.63391095 -0.11146313\n",
      " -1.27504585 -1.56594153 -0.40235881 -0.98415017  1.34301527  0.76122391]\n",
      "Unique values in Cluster: [ 2.16818699  0.03463184  1.10140941 -1.03214573  3.23496456]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in Sale_Month:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Sale_Month after processing: [ 8  7 10  4 12  6  2  1  5  3 11  9]\n",
      "Unique values in Cluster after processing: [3. 1. 2. 0. 4.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 恢復原始數據\n",
    "data = final_data.copy()  # 使用未經標準化的原始數據進行重新處理\n",
    "\n",
    "# 填充缺失值\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# 確認不需要標準化的特徵（例如 Sale_Month 和 Cluster）\n",
    "exclude_columns = ['Sale_Month', 'Cluster']\n",
    "numeric_columns_for_scaling = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "# 對需要標準化的數值特徵進行標準化\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns_for_scaling] = scaler.fit_transform(data[numeric_columns_for_scaling])\n",
    "\n",
    "# 確認 Sale_Month 和 Cluster 列保持原始值\n",
    "print(\"Unique values in Sale_Month after processing:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster after processing:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Cluster after conversion: [3 1 2 0 4]\n"
     ]
    }
   ],
   "source": [
    "# 將 Cluster 列轉換為整數型\n",
    "data['Cluster'] = data['Cluster'].astype(int)\n",
    "\n",
    "# 檢查轉換後的值\n",
    "print(\"Unique values in Cluster after conversion:\", data['Cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除極端值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after adjusting IQR threshold: (126775, 176)\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers_iqr(df, columns, iqr_multiplier=1.5):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "        df[column] = np.where((df[column] < lower_bound) | (df[column] > upper_bound), np.nan, df[column])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用更大的 IQR 邊界（例如 2 或 2.5）去除極端值\n",
    "data_cleaned = remove_outliers_iqr(data, numeric_columns, iqr_multiplier=1.5)\n",
    "print(\"Data shape after adjusting IQR threshold:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after removing outliers with Z-score: (125429, 176)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers_zscore(df, columns, z_threshold=3):\n",
    "    for column in columns:\n",
    "        # 計算 Z-score，並保持原始數據的形狀\n",
    "        z_scores = np.abs(stats.zscore(df[column].fillna(df[column].median())))  # 填充缺失值以便計算 Z-score\n",
    "        # 將 Z-score 超過閾值的數值標記為 NaN\n",
    "        df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n",
    "    # 刪除包含 NaN 的行\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用 Z-score 方法去除異常值\n",
    "data_cleaned = remove_outliers_zscore(data, numeric_columns, z_threshold=3)\n",
    "print(\"Data shape after removing outliers with Z-score:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after stricter outlier removal for key features: (113135, 176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/r38sgwz531dgm9my5q3lc2t00000gn/T/ipykernel_44415/1221692749.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n"
     ]
    }
   ],
   "source": [
    "# 針對特定特徵使用更嚴格的異常值閾值\n",
    "important_columns = ['PRICE', 'ROOMS', 'GBA']  # 替換為您認為重要的特徵\n",
    "data_important_cleaned = remove_outliers_iqr(data, important_columns, iqr_multiplier=1.5)\n",
    "data_important_cleaned = remove_outliers_zscore(data_important_cleaned, important_columns, z_threshold=2.5)\n",
    "print(\"Data shape after stricter outlier removal for key features:\", data_important_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after quantile clipping: (162457, 176)\n"
     ]
    }
   ],
   "source": [
    "def quantile_clipping(df, columns, lower_quantile=0.025, upper_quantile=0.975):\n",
    "    for column in columns:\n",
    "        lower_bound = df[column].quantile(lower_quantile)\n",
    "        upper_bound = df[column].quantile(upper_quantile)\n",
    "        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "# 使用分位數截斷法去除異常值\n",
    "data_clipped = quantile_clipping(data, numeric_columns)\n",
    "print(\"Data shape after quantile clipping:\", data_clipped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理缺失值（NaN 值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after initial processing: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Initial NaN handling for numeric and categorical columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# 確認缺失值已處理\n",
    "print(\"Missing values after initial processing:\", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化與異常值處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Detect and handle outliers using Z-score with an adjusted threshold\n",
    "z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "data[numeric_columns] = np.where(z_scores > 2.5, np.nan, data[numeric_columns])\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割數據集並進行最終的 NaN 填補"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transform target variable and split dataset\n",
    "y = np.log1p(data['PRICE'])  # Apply log transformation to target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "\n",
    "# 確保只有數值列被用於 PCA\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fill any remaining NaN values with median in both training and test sets\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PCA 降維"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature count: 162\n",
      "Reduced feature count with PCA: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=30)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Original feature count:\", X_train.shape[1])\n",
    "print(\"Reduced feature count with PCA:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in y_train: 36416\n",
      "NaN values in y_test: 9023\n",
      "NaN values in y_pred_lr: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN values in y_train:\", y_train.isnull().sum())\n",
    "print(\"NaN values in y_test:\", np.isnan(y_test).sum())\n",
    "print(\"NaN values in y_pred_lr:\", np.isnan(y_pred_lr).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in y_train after processing: 0\n",
      "NaN values in y_test after processing: 0\n",
      "NaN values in y_pred_lr after processing: 0\n"
     ]
    }
   ],
   "source": [
    "# 處理 y_train 和 y_test 中的 NaN 值\n",
    "y_train.fillna(y_train.median(), inplace=True)\n",
    "y_test = y_test.fillna(y_test.median())\n",
    "print(\"NaN values in y_train after processing:\", y_train.isnull().sum())\n",
    "print(\"NaN values in y_test after processing:\", np.isnan(y_test).sum())\n",
    "\n",
    "# 處理 y_pred_lr 中的 NaN 值\n",
    "y_pred_lr = np.nan_to_num(y_pred_lr, nan=np.mean(y_pred_lr))\n",
    "print(\"NaN values in y_pred_lr after processing:\", np.isnan(y_pred_lr).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練和評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 0.5433593670207059 R^2: 0.330446982360443\n",
      "Random Forest RMSE: 0.2031341384585862 R^2: 0.9064214119450209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Train and evaluate Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_pca, y_train)\n",
    "y_pred_lr = np.expm1(linear_model.predict(X_test_pca))\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_pca, y_train)\n",
    "y_pred_rf = np.expm1(rf_model.predict(X_test_pca))\n",
    "\n",
    "# Calculate RMSE and R^2 for both models\n",
    "rmse_lr = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_lr))\n",
    "r2_lr = r2_score(np.expm1(y_test), y_pred_lr)\n",
    "rmse_rf = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_rf))\n",
    "r2_rf = r2_score(np.expm1(y_test), y_pred_rf)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", rmse_lr, \"R^2:\", r2_lr)\n",
    "print(\"Random Forest RMSE:\", rmse_rf, \"R^2:\", r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
