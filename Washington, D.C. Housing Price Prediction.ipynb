{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Computer_Assisted_Mass_Appraisal_-_Residential.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical columns for One-Hot Encoding\n",
    "categorical_features = ['HEAT', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'USECODE']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "onehot_encoder = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=int), categorical_features)], remainder='passthrough')\n",
    "data_encoded = onehot_encoder.fit_transform(data)\n",
    "\n",
    "# Update column names for one-hot encoded features\n",
    "encoded_feature_names = onehot_encoder.named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "data_encoded_df = pd.DataFrame(data_encoded, columns=list(encoded_feature_names) + list(data.columns.drop(categorical_features)))\n",
    "\n",
    "# Handle zero values in certain columns to prevent division errors in interaction features\n",
    "data_encoded_df = data_encoded_df.replace({'LANDAREA': {0: np.nan}, 'ROOMS': {0: np.nan}})\n",
    "\n",
    "# Create interaction features\n",
    "data_encoded_df['Rooms_Bathrooms'] = data_encoded_df['ROOMS'] * data_encoded_df['BATHRM']\n",
    "data_encoded_df['Building_Density'] = data_encoded_df['GBA'] / data_encoded_df['LANDAREA']\n",
    "data_encoded_df['Bedroom_Room_Ratio'] = data_encoded_df['BEDRM'] / data_encoded_df['ROOMS']\n",
    "\n",
    "# Fill NaN values after division\n",
    "data_encoded_df = data_encoded_df.fillna({'Building_Density': 0, 'Bedroom_Room_Ratio': 0})\n",
    "\n",
    "# Handle invalid values\n",
    "data_encoded_df = data_encoded_df[\n",
    "    (data_encoded_df['YR_RMDL'] >= 1000) & (data_encoded_df['YR_RMDL'] <= 2023) &\n",
    "    (data_encoded_df['AYB'] >= 1000) & (data_encoded_df['AYB'] <= 2023) &\n",
    "    (data_encoded_df['EYB'] >= 1000) & (data_encoded_df['EYB'] <= 2023) &\n",
    "    (data_encoded_df['GBA'] > 0) & (data_encoded_df['LANDAREA'] > 0) \n",
    "]\n",
    "\n",
    "\n",
    "# Convert SALEDATE to datetime format and derive date-based features\n",
    "data_encoded_df['SALEDATE'] = pd.to_datetime(data_encoded_df['SALEDATE'])\n",
    "data_encoded_df['Property_Age'] = 2023 - data_encoded_df['AYB']\n",
    "data_encoded_df['Years_Since_Remodel'] = 2023 - data_encoded_df['YR_RMDL']\n",
    "data_encoded_df['Years_Between_Built_and_Remodel'] = data_encoded_df['YR_RMDL'] - data_encoded_df['AYB']\n",
    "data_encoded_df['Sale_Year'] = data_encoded_df['SALEDATE'].dt.year\n",
    "data_encoded_df['Sale_Month'] = data_encoded_df['SALEDATE'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand data by clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering (using numeric features only for simplicity)\n",
    "features_for_clustering = ['ROOMS', 'BATHRM', 'LANDAREA', 'GBA', 'PRICE']\n",
    "data_cluster = data_encoded_df[features_for_clustering].dropna()\n",
    "\n",
    "# Preserve original index for later merge\n",
    "data_cluster = data_cluster.reset_index()  # This adds the original index as a column\n",
    "\n",
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cluster[features_for_clustering])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "n_clusters = 5  # Set an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "data_cluster['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Merge cluster labels back to original data\n",
    "data_encoded_df = data_encoded_df.merge(data_cluster[['index', 'Cluster']], left_index=True, right_on='index', how='left')\n",
    "data_encoded_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Remove rows with NaN in Cluster column for SMOTE application\n",
    "data_with_clusters = data_encoded_df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Placeholder for augmented data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Apply random oversampling within each cluster\n",
    "for cluster in data_with_clusters['Cluster'].unique():\n",
    "    # Select data for the current cluster\n",
    "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "    \n",
    "    # Set the desired size for oversampling (e.g., double the original size of each cluster)\n",
    "    target_size = len(cluster_data) * 2\n",
    "    \n",
    "    # Perform random oversampling\n",
    "    cluster_augmented = resample(cluster_data, replace=True, n_samples=target_size, random_state=42)\n",
    "    \n",
    "    # Append to augmented data\n",
    "    augmented_data = pd.concat([augmented_data, cluster_augmented])\n",
    "\n",
    "# Combine original data with augmented data\n",
    "final_data = pd.concat([data_encoded_df, augmented_data], ignore_index=True)\n",
    "\n",
    "# Check final dataset shape and display a sample\n",
    "print(\"Original Data Shape (before encoding):\", data.shape)\n",
    "print(\"Data Encoded Shape:\", data_encoded_df.shape)\n",
    "print(\"Augmented Data Shape:\", augmented_data.shape)\n",
    "print(\"Final Data Shape:\", final_data.shape)\n",
    "final_data.head()\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---  Below Unused ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_data.copy()\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Fill missing values in numeric columns with the median\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check that missing values have been handled\n",
    "print(\"Missing values after processing:\", data.isnull().sum().sum())\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "# Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Step 3: Detect and handle outliers\n",
    "# # Use Z-score method to detect outliers; replace values with NaN if Z-score > 3\n",
    "# z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "# data[numeric_columns] = np.where(z_scores > 3, np.nan, data[numeric_columns])\n",
    "# # Fill the NaN values resulting from outliers with the median\n",
    "# data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "# Assuming 'PRICE' is the target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "\n",
    "# Ensure that only numeric columns are included in X for model training\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Problematic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for each numeric feature (including Q1, Q2 (median), Q3)\n",
    "numeric_data = data.drop(columns=categorical_columns)\n",
    "numeric_summary = numeric_data.describe(percentiles=[0.25, 0.5, 0.75]).T  # Transpose for better readability\n",
    "numeric_summary['IQR'] = numeric_summary['75%'] - numeric_summary['25%']  # Calculate IQR\n",
    "\n",
    "print(\"\\nNumeric Features Summary (including Q1, Q2, Q3, IQR):\\n\", numeric_summary)\n",
    "\n",
    "# Check percentage of extreme values (outliers)\n",
    "# Calculate percentage of outliers where Z-score exceeds threshold\n",
    "outliers_percentage = {}\n",
    "\n",
    "for col in numeric_data.columns:\n",
    "    z_scores = stats.zscore(numeric_data[col].dropna())  # Exclude missing values and calculate Z-score\n",
    "    outliers_percentage[col] = (np.abs(z_scores) > 3).mean() * 100  # Calculate the percentage of outliers where Z-score > 3\n",
    "\n",
    "outliers_percentage_df = pd.DataFrame.from_dict(outliers_percentage, orient='index', columns=['Outliers (%)'])\n",
    "print(\"\\nPercentage of Outliers in Each Numeric Feature:\\n\", outliers_percentage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of One-Hot Encoded features\n",
    "one_hot_columns = [col for col in data.columns if 'HEAT_' in col or 'STYLE_' in col or 'STRUCT_' in col]  # Replace with the prefixes of features you used for One-Hot Encoding\n",
    "for col in one_hot_columns:\n",
    "    print(f\"{col} unique values:\", data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years_Since_Remodel'] = data['Years_Since_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "data['Years_Between_Built_and_Remodel'] = data['Years_Between_Built_and_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "# fill NaN with median\n",
    "data['Years_Since_Remodel'].fillna(data['Years_Since_Remodel'].median(), inplace=True)\n",
    "data['Years_Between_Built_and_Remodel'].fillna(data['Years_Between_Built_and_Remodel'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in Sale_Month:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster:\", data['Cluster'].unique())\n",
    "print(\"Unique values in Years_Since_Remodel:\", data['Years_Since_Remodel'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Above Unused ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[categorical_columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_data.copy()\n",
    "\n",
    "# fill missing values\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# standardize numeric features\n",
    "exclude_columns = ['Sale_Month', 'Cluster']\n",
    "numeric_columns_for_scaling = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns_for_scaling] = scaler.fit_transform(data[numeric_columns_for_scaling])\n",
    "\n",
    "# Check data\n",
    "print(\"Unique values in Sale_Month after processing:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster after processing:\", data['Cluster'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type to int\n",
    "data['Cluster'] = data['Cluster'].astype(int)\n",
    "\n",
    "# Check data\n",
    "print(\"Unique values in Cluster after conversion:\", data['Cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns, iqr_multiplier=1.5):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "        df[column] = np.where((df[column] < lower_bound) | (df[column] > upper_bound), np.nan, df[column])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "data_cleaned = remove_outliers_iqr(data, numeric_columns, iqr_multiplier=1.5)\n",
    "print(\"Data shape after adjusting IQR threshold:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(df, columns, z_threshold=3):\n",
    "    for column in columns:\n",
    "        # Calculate Z-score and replace values with NaN if Z-score > threshold\n",
    "        z_scores = np.abs(stats.zscore(df[column].fillna(df[column].median())))  # Fill NaN values with median\n",
    "        # Replace values with NaN if Z-score > threshold\n",
    "        df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "data_cleaned = remove_outliers_zscore(data, numeric_columns, z_threshold=3)\n",
    "print(\"Data shape after removing outliers with Z-score:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers for key features\n",
    "important_columns = ['PRICE', 'ROOMS', 'GBA']  # Define important columns for outlier removal\n",
    "data_important_cleaned = remove_outliers_iqr(data, important_columns, iqr_multiplier=1.5)\n",
    "data_important_cleaned = remove_outliers_zscore(data_important_cleaned, important_columns, z_threshold=2.5)\n",
    "print(\"Data shape after stricter outlier removal for key features:\", data_important_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_clipping(df, columns, lower_quantile=0.025, upper_quantile=0.975):\n",
    "    for column in columns:\n",
    "        lower_bound = df[column].quantile(lower_quantile)\n",
    "        upper_bound = df[column].quantile(upper_quantile)\n",
    "        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "# Apply quantile clipping to the entire dataset\n",
    "data_clipped = quantile_clipping(data, numeric_columns)\n",
    "print(\"Data shape after quantile clipping:\", data_clipped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial NaN handling for numeric and categorical columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check missing values after initial processing\n",
    "print(\"Missing values after initial processing:\", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Detect and handle outliers using Z-score with an adjusted threshold\n",
    "z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "data[numeric_columns] = np.where(z_scores > 2.5, np.nan, data[numeric_columns])\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target variable and split dataset\n",
    "y = np.log1p(data['PRICE'])  # Apply log transformation to target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "\n",
    "# Ensure that only numeric columns are included in X for model training\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fill any remaining NaN values with median in both training and test sets\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA to Reduce Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Original feature count:\", X_train.shape[1])\n",
    "print(\"Reduced feature count with PCA:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaN values in y_train:\", y_train.isnull().sum())\n",
    "print(\"NaN values in y_test:\", np.isnan(y_test).sum())\n",
    "# print(\"NaN values in y_pred_lr:\", np.isnan(y_pred_lr).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in y_train and y_test with median\n",
    "y_train.fillna(y_train.median(), inplace=True)\n",
    "y_test = y_test.fillna(y_test.median())\n",
    "print(\"NaN values in y_train after processing:\", y_train.isnull().sum())\n",
    "print(\"NaN values in y_test after processing:\", np.isnan(y_test).sum())\n",
    "\n",
    "# # Fill NaN values in y_pred_lr with the mean\n",
    "# y_pred_lr = np.nan_to_num(y_pred_lr, nan=np.mean(y_pred_lr))\n",
    "# print(\"NaN values in y_pred_lr after processing:\", np.isnan(y_pred_lr).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_pca, y_train)\n",
    "y_pred_lr = np.expm1(linear_model.predict(X_test_pca))\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_pca, y_train)\n",
    "y_pred_rf = np.expm1(rf_model.predict(X_test_pca))\n",
    "\n",
    "# Calculate RMSE and R^2 for both models\n",
    "rmse_lr = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_lr))\n",
    "r2_lr = r2_score(np.expm1(y_test), y_pred_lr)\n",
    "rmse_rf = np.sqrt(mean_squared_error(np.expm1(y_test), y_pred_rf))\n",
    "r2_rf = r2_score(np.expm1(y_test), y_pred_rf)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", rmse_lr, \"R^2:\", r2_lr)\n",
    "print(\"Random Forest RMSE:\", rmse_rf, \"R^2:\", r2_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
