{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Computer_Assisted_Mass_Appraisal_-_Residential.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical columns for One-Hot Encoding\n",
    "categorical_features = ['HEAT', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'USECODE']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "onehot_encoder = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_features)], remainder='passthrough')\n",
    "data_encoded = onehot_encoder.fit_transform(data)\n",
    "\n",
    "# Update column names for one-hot encoded features\n",
    "encoded_feature_names = onehot_encoder.named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "data_encoded_df = pd.DataFrame(data_encoded, columns=list(encoded_feature_names) + list(data.columns.drop(categorical_features)))\n",
    "\n",
    "# Handle zero values in certain columns to prevent division errors in interaction features\n",
    "data_encoded_df['LANDAREA'].replace(0, np.nan, inplace=True)\n",
    "data_encoded_df['ROOMS'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "data_encoded_df['Rooms_Bathrooms'] = data_encoded_df['ROOMS'] * data_encoded_df['BATHRM']\n",
    "data_encoded_df['Building_Density'] = data_encoded_df['GBA'] / data_encoded_df['LANDAREA']\n",
    "data_encoded_df['Bedroom_Room_Ratio'] = data_encoded_df['BEDRM'] / data_encoded_df['ROOMS']\n",
    "\n",
    "# Fill NaN values after division\n",
    "data_encoded_df['Building_Density'].fillna(0, inplace=True)\n",
    "data_encoded_df['Bedroom_Room_Ratio'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert SALEDATE to datetime format and derive date-based features\n",
    "data_encoded_df['SALEDATE'] = pd.to_datetime(data_encoded_df['SALEDATE'])\n",
    "data_encoded_df['Property_Age'] = 2023 - data_encoded_df['AYB']\n",
    "data_encoded_df['Years_Since_Remodel'] = 2023 - data_encoded_df['YR_RMDL']\n",
    "data_encoded_df['Years_Between_Built_and_Remodel'] = data_encoded_df['YR_RMDL'] - data_encoded_df['AYB']\n",
    "data_encoded_df['Sale_Year'] = data_encoded_df['SALEDATE'].dt.year\n",
    "data_encoded_df['Sale_Month'] = data_encoded_df['SALEDATE'].dt.month\n",
    "\n",
    "# Define features for clustering (using numeric features only for simplicity)\n",
    "features_for_clustering = ['ROOMS', 'BATHRM', 'LANDAREA', 'GBA', 'PRICE']\n",
    "data_cluster = data_encoded_df[features_for_clustering].dropna()\n",
    "\n",
    "# Preserve original index for later merge\n",
    "data_cluster = data_cluster.reset_index()  # This adds the original index as a column\n",
    "\n",
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cluster[features_for_clustering])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "n_clusters = 5  # Set an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "data_cluster['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Merge cluster labels back to original data\n",
    "data_encoded_df = data_encoded_df.merge(data_cluster[['index', 'Cluster']], left_index=True, right_on='index', how='left')\n",
    "data_encoded_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Remove rows with NaN in Cluster column for SMOTE application\n",
    "data_with_clusters = data_encoded_df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Placeholder for augmented data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Apply random oversampling within each cluster\n",
    "for cluster in data_with_clusters['Cluster'].unique():\n",
    "    # Select data for the current cluster\n",
    "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "    \n",
    "    # Set the desired size for oversampling (e.g., double the original size of each cluster)\n",
    "    target_size = len(cluster_data) * 2\n",
    "    \n",
    "    # Perform random oversampling\n",
    "    cluster_augmented = resample(cluster_data, replace=True, n_samples=target_size, random_state=42)\n",
    "    \n",
    "    # Append to augmented data\n",
    "    augmented_data = pd.concat([augmented_data, cluster_augmented])\n",
    "\n",
    "# Combine original data with augmented data\n",
    "final_data = pd.concat([data_encoded_df, augmented_data], ignore_index=True)\n",
    "\n",
    "# Check final dataset shape and display a sample\n",
    "print(\"Original Data Shape (before encoding):\", data.shape)\n",
    "print(\"Data Encoded Shape:\", data_encoded_df.shape)\n",
    "print(\"Augmented Data Shape:\", augmented_data.shape)\n",
    "print(\"Final Data Shape:\", final_data.shape)\n",
    "final_data.head()\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming final_data is the augmented dataset\n",
    "data = final_data.copy()\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Fill missing values in numeric columns with the median\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check that missing values have been handled\n",
    "print(\"Missing values after processing:\", data.isnull().sum().sum())\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "# Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Step 3: Detect and handle outliers\n",
    "# Use Z-score method to detect outliers; replace values with NaN if Z-score > 3\n",
    "z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "data[numeric_columns] = np.where(z_scores > 3, np.nan, data[numeric_columns])\n",
    "# Fill the NaN values resulting from outliers with the median\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "# Assuming 'PRICE' is the target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "\n",
    "# Ensure that only numeric columns are included in X for model training\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理有問題的數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 檢查每個特徵的數據類型\n",
    "print(\"Data Types for Each Feature:\\n\", data.dtypes)\n",
    "\n",
    "# 計算每個數值特徵的描述統計量（包括 Q1, Q2 (中位數), Q3）\n",
    "numeric_summary = data.describe(percentiles=[0.25, 0.5, 0.75]).T  # 使用轉置方便閱讀\n",
    "numeric_summary['IQR'] = numeric_summary['75%'] - numeric_summary['25%']  # 計算 IQR\n",
    "\n",
    "# 顯示每個特徵的四分位數信息\n",
    "print(\"\\nNumeric Features Summary (including Q1, Q2, Q3, IQR):\\n\", numeric_summary)\n",
    "\n",
    "# 檢查極端值（異常值）百分比\n",
    "# 計算 Z-score 超過閾值的異常值比例\n",
    "from scipy import stats\n",
    "outliers_percentage = {}\n",
    "\n",
    "for col in data.select_dtypes(include=[np.number]).columns:\n",
    "    z_scores = stats.zscore(data[col].dropna())  # 排除缺失值，計算 Z-score\n",
    "    outliers_percentage[col] = (np.abs(z_scores) > 3).mean() * 100  # 計算 Z-score > 3 的異常值比例\n",
    "\n",
    "outliers_percentage_df = pd.DataFrame.from_dict(outliers_percentage, orient='index', columns=['Outliers (%)'])\n",
    "print(\"\\nPercentage of Outliers in Each Numeric Feature:\\n\", outliers_percentage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 One-Hot Encoding 特徵的唯一值\n",
    "one_hot_columns = [col for col in data.columns if 'HEAT_' in col or 'STYLE_' in col or 'STRUCT_' in col]  # 替換為您使用 One-Hot Encoding 的特徵前綴\n",
    "for col in one_hot_columns:\n",
    "    print(f\"{col} unique values:\", data[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years_Since_Remodel'] = data['Years_Since_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "data['Years_Between_Built_and_Remodel'] = data['Years_Between_Built_and_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "# 用中位數填補 NaN\n",
    "data['Years_Since_Remodel'].fillna(data['Years_Since_Remodel'].median(), inplace=True)\n",
    "data['Years_Between_Built_and_Remodel'].fillna(data['Years_Between_Built_and_Remodel'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in Sale_Month:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 恢復原始數據\n",
    "data = final_data.copy()  # 使用未經標準化的原始數據進行重新處理\n",
    "\n",
    "# 填充缺失值\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# 確認不需要標準化的特徵（例如 Sale_Month 和 Cluster）\n",
    "exclude_columns = ['Sale_Month', 'Cluster']\n",
    "numeric_columns_for_scaling = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "# 對需要標準化的數值特徵進行標準化\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns_for_scaling] = scaler.fit_transform(data[numeric_columns_for_scaling])\n",
    "\n",
    "# 確認 Sale_Month 和 Cluster 列保持原始值\n",
    "print(\"Unique values in Sale_Month after processing:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster after processing:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 Cluster 列轉換為整數型\n",
    "data['Cluster'] = data['Cluster'].astype(int)\n",
    "\n",
    "# 檢查轉換後的值\n",
    "print(\"Unique values in Cluster after conversion:\", data['Cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除極端值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns, iqr_multiplier=1.5):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "        df[column] = np.where((df[column] < lower_bound) | (df[column] > upper_bound), np.nan, df[column])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用更大的 IQR 邊界（例如 2 或 2.5）去除極端值\n",
    "data_cleaned = remove_outliers_iqr(data, numeric_columns, iqr_multiplier=1.5)\n",
    "print(\"Data shape after adjusting IQR threshold:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers_zscore(df, columns, z_threshold=3):\n",
    "    for column in columns:\n",
    "        # 計算 Z-score，並保持原始數據的形狀\n",
    "        z_scores = np.abs(stats.zscore(df[column].fillna(df[column].median())))  # 填充缺失值以便計算 Z-score\n",
    "        # 將 Z-score 超過閾值的數值標記為 NaN\n",
    "        df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n",
    "    # 刪除包含 NaN 的行\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用 Z-score 方法去除異常值\n",
    "data_cleaned = remove_outliers_zscore(data, numeric_columns, z_threshold=3)\n",
    "print(\"Data shape after removing outliers with Z-score:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 針對特定特徵使用更嚴格的異常值閾值\n",
    "important_columns = ['PRICE', 'ROOMS', 'GBA']  # 替換為您認為重要的特徵\n",
    "data_important_cleaned = remove_outliers_iqr(data, important_columns, iqr_multiplier=1.5)\n",
    "data_important_cleaned = remove_outliers_zscore(data_important_cleaned, important_columns, z_threshold=2.5)\n",
    "print(\"Data shape after stricter outlier removal for key features:\", data_important_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_clipping(df, columns, lower_quantile=0.025, upper_quantile=0.975):\n",
    "    for column in columns:\n",
    "        lower_bound = df[column].quantile(lower_quantile)\n",
    "        upper_bound = df[column].quantile(upper_quantile)\n",
    "        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "# 使用分位數截斷法去除異常值\n",
    "data_clipped = quantile_clipping(data, numeric_columns)\n",
    "print(\"Data shape after quantile clipping:\", data_clipped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將數據分割為訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 data 是否有數據\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# 確認 'PRICE' 列是否存在\n",
    "if 'PRICE' not in data.columns:\n",
    "    print(\"'PRICE' column not found in data.\")\n",
    "else:\n",
    "    print(\"PRICE column found with shape:\", data['PRICE'].shape)\n",
    "\n",
    "# 確認 X 和 y 的形狀\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "print(\"X shape before splitting:\", X.shape)\n",
    "print(\"y shape before splitting:\", y.shape)\n",
    "\n",
    "# 分割數據集（在確認 X 和 y 有數據之後再執行）\n",
    "if X.shape[0] > 0 and y.shape[0] > 0:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "else:\n",
    "    print(\"X or y has no data, please check your preprocessing steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有特徵列轉換為數值類型，無法轉換的值設為 NaN\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "\n",
    "# 檢查轉換後是否還有 NaN 值\n",
    "nan_counts_train = X_train.isnull().sum()\n",
    "nan_counts_test = X_test.isnull().sum()\n",
    "\n",
    "print(\"Number of NaNs in each column of X_train after conversion:\\n\", nan_counts_train[nan_counts_train > 0])\n",
    "print(\"Number of NaNs in each column of X_test after conversion:\\n\", nan_counts_test[nan_counts_test > 0])\n",
    "\n",
    "# 用中位數填補 NaN 值（或根據需要選擇其他填補策略）\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除含有大量 NaN 值的欄位（如果不必要）\n",
    "columns_to_drop = ['SSL', 'HEAT_D', 'AC', 'STYLE_D', 'STRUCT_D', 'GRADE_D', 'CNDTN_D', 'EXTWALL_D', 'ROOF_D', \n",
    "                   'INTWALL_D', 'GIS_LAST_MOD_DTTM', 'Sale_Year', 'Cluster',  'QUALIFIED']  # 根據需要調整列表\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n",
    "X_test = X_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認沒有 NaN 值\n",
    "print(\"Number of NaNs in each column of X_train after filling:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in each column of X_test after filling:\\n\", X_test.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看每個欄位中剩餘的 NaN 數量\n",
    "print(\"Remaining NaN values in each column of X_train:\\n\", X_train.isnull().sum().sort_values(ascending=False).head(20))\n",
    "print(\"Remaining NaN values in each column of X_test:\\n\", X_test.isnull().sum().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Are there any NaN values in X_train?\", np.isnan(X_train).any())\n",
    "print(\"Are there any NaN values in X_test?\", np.isnan(X_test).any())\n",
    "print(\"Are there any Infinity values in X_train?\", np.isinf(X_train).any())\n",
    "print(\"Are there any Infinity values in X_test?\", np.isinf(X_test).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有的 Infinity 值替換為 NaN\n",
    "X_train = np.where(np.isinf(X_train), np.nan, X_train)\n",
    "X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "\n",
    "# 用中位數替換 NaN 值\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum value in X_train:\", np.nanmax(X_train))\n",
    "print(\"Minimum value in X_train:\", np.nanmin(X_train))\n",
    "print(\"Maximum value in X_test:\", np.nanmax(X_test))\n",
    "print(\"Minimum value in X_test:\", np.nanmin(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數值限制在一個合理的範圍\n",
    "X_train = np.clip(X_train, -1e6, 1e6)\n",
    "X_test = np.clip(X_test, -1e6, 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 逐列檢查 NaN 和 Infinity\n",
    "for column in range(X_train.shape[1]):\n",
    "    if np.isnan(X_train[:, column]).any():\n",
    "        print(f\"NaN found in column {column} of X_train\")\n",
    "    if np.isinf(X_train[:, column]).any():\n",
    "        print(f\"Infinity found in column {column} of X_train\")\n",
    "\n",
    "for column in range(X_test.shape[1]):\n",
    "    if np.isnan(X_test[:, column]).any():\n",
    "        print(f\"NaN found in column {column} of X_test\")\n",
    "    if np.isinf(X_test[:, column]).any():\n",
    "        print(f\"Infinity found in column {column} of X_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum value in X_train:\", np.max(X_train))\n",
    "print(\"Minimum value in X_train:\", np.min(X_train))\n",
    "print(\"Maximum value in X_test:\", np.max(X_test))\n",
    "print(\"Minimum value in X_test:\", np.min(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 將 X_train 和 X_test 轉回 DataFrame 格式\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# 檢查數據類型\n",
    "print(\"Data types in X_train:\\n\", X_train.dtypes)\n",
    "print(\"Data types in X_test:\\n\", X_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認每一列中是否含有非數值的字符\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].apply(lambda x: isinstance(x, str)).any():\n",
    "        print(f\"Column {col} contains non-numeric values in X_train\")\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].apply(lambda x: isinstance(x, str)).any():\n",
    "        print(f\"Column {col} contains non-numeric values in X_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 強制轉換為 float64，無法轉換的設為 NaN\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 再次用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 訓練線性回歸模型\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測並評估\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_lr)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_lr)\n",
    "print(\"R-squared (R²):\", r2_lr)\n",
    "\n",
    "# 訓練隨機森林模型\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測並評估\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_rf)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_rf)\n",
    "print(\"R-squared (R²):\", r2_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: 將 Infinity 值替換為 NaN（適用於 numpy.ndarray）\n",
    "X_train[np.isinf(X_train)] = np.nan\n",
    "X_test[np.isinf(X_test)] = np.nan\n",
    "\n",
    "# Step 2: 使用中位數填補 NaN 值\n",
    "# 因為是 numpy.ndarray 格式，需計算中位數後再進行填補\n",
    "# 計算每列的中位數，並將 NaN 值替換為中位數\n",
    "col_medians_train = np.nanmedian(X_train, axis=0)\n",
    "col_medians_test = np.nanmedian(X_test, axis=0)\n",
    "\n",
    "# 使用中位數填補 NaN 值\n",
    "inds_train = np.where(np.isnan(X_train))\n",
    "inds_test = np.where(np.isnan(X_test))\n",
    "\n",
    "X_train[inds_train] = np.take(col_medians_train, inds_train[1])\n",
    "X_test[inds_test] = np.take(col_medians_test, inds_test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再度確認有沒有空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 檢查 NaN 和 Infinity 值\n",
    "print(\"Are there any NaN values in X_train?\", np.isnan(X_train).any())\n",
    "print(\"Are there any NaN values in X_test?\", np.isnan(X_test).any())\n",
    "print(\"Are there any Infinity values in X_train?\", np.isinf(X_train).any())\n",
    "print(\"Are there any Infinity values in X_test?\", np.isinf(X_test).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 X_train 和 X_test 中的最大和最小值\n",
    "print(\"Maximum value in X_train:\", np.nanmax(X_train))\n",
    "print(\"Minimum value in X_train:\", np.nanmin(X_train))\n",
    "print(\"Maximum value in X_test:\", np.nanmax(X_test))\n",
    "print(\"Minimum value in X_test:\", np.nanmin(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查所有特徵的數據類型\n",
    "print(\"Data Types for Each Feature in X:\\n\", X.dtypes)\n",
    "\n",
    "# 找出非數值列\n",
    "non_numeric_columns = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除無法轉換為數值的列（如不必要的字符串列）\n",
    "X = X.drop(columns=non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有列強制轉換為數值類型，無法轉換的值設為 NaN\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 再次檢查是否有 NaN 並填補\n",
    "X = X.fillna(X.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認標準化之前所有列的數據類型\n",
    "print(\"Data types in X_train before standardization:\\n\", pd.DataFrame(X_train).dtypes)\n",
    "print(\"Data types in X_test before standardization:\\n\", pd.DataFrame(X_test).dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假設您已經有原始的列名\n",
    "X_train = pd.DataFrame(X_train, columns= X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns= X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 強制將所有列轉換為數值類型，無法轉換的值設置為 NaN\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 檢查是否有 NaN 值\n",
    "print(\"Number of NaNs in X_train after conversion:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in X_test after conversion:\\n\", X_test.isnull().sum().sum())\n",
    "\n",
    "# 使用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認所有列的數據類型\n",
    "print(\"Data types in X_train after conversion:\\n\", X_train.dtypes)\n",
    "print(\"Data types in X_test after conversion:\\n\", X_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    non_numeric_count = X_train[col].apply(lambda x: isinstance(x, (str, bytes))).sum()\n",
    "    if non_numeric_count > 0:\n",
    "        print(f\"Column {col} has {non_numeric_count} non-numeric values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NaNs in X_train after conversion:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in X_test after conversion:\\n\", X_test.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 標準化數據\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化線性回歸模型\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# 訓練模型\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# 評估模型表現\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_lr)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_lr)\n",
    "print(\"R-squared (R²):\", r2_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 初始化並訓練隨機森林模型\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 計算隨機森林模型的評估指標\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_rf)\n",
    "print(\"R-squared (R²):\", r2_rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
