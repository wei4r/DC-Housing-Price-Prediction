{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (before encoding): (109034, 39)\n",
      "Data Encoded Shape: (109034, 176)\n",
      "Augmented Data Shape: (189314, 176)\n",
      "Final Data Shape: (298348, 176)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>LANDAREA</th>\n",
       "      <th>Building_Density</th>\n",
       "      <th>Bedroom_Room_Ratio</th>\n",
       "      <th>Sale_Year</th>\n",
       "      <th>Sale_Month</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>296126.000000</td>\n",
       "      <td>298310.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>283971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.498886</td>\n",
       "      <td>3356.172287</td>\n",
       "      <td>0.742864</td>\n",
       "      <td>0.468994</td>\n",
       "      <td>2005.016424</td>\n",
       "      <td>6.182840</td>\n",
       "      <td>0.888629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.333140</td>\n",
       "      <td>5673.380941</td>\n",
       "      <td>0.459735</td>\n",
       "      <td>0.134558</td>\n",
       "      <td>31.018729</td>\n",
       "      <td>3.537094</td>\n",
       "      <td>0.899906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>0.397614</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2313.000000</td>\n",
       "      <td>0.635209</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>4140.000000</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>942632.000000</td>\n",
       "      <td>4.988943</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ROOMS       LANDAREA  Building_Density  Bedroom_Room_Ratio  \\\n",
       "count  296126.000000  298310.000000     298348.000000       298348.000000   \n",
       "mean        7.498886    3356.172287          0.742864            0.468994   \n",
       "std         2.333140    5673.380941          0.459735            0.134558   \n",
       "min         1.000000       1.000000          0.000000            0.000000   \n",
       "25%         6.000000    1571.000000          0.397614            0.400000   \n",
       "50%         7.000000    2313.000000          0.635209            0.500000   \n",
       "75%         8.000000    4140.000000          0.988235            0.500000   \n",
       "max        48.000000  942632.000000          4.988943            8.000000   \n",
       "\n",
       "           Sale_Year     Sale_Month        Cluster  \n",
       "count  298348.000000  298348.000000  283971.000000  \n",
       "mean     2005.016424       6.182840       0.888629  \n",
       "std        31.018729       3.537094       0.899906  \n",
       "min      1900.000000       1.000000       0.000000  \n",
       "25%      2005.000000       3.000000       0.000000  \n",
       "50%      2015.000000       6.000000       1.000000  \n",
       "75%      2020.000000       9.000000       1.000000  \n",
       "max      2024.000000      12.000000       4.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Computer_Assisted_Mass_Appraisal_-_Residential.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical columns for One-Hot Encoding\n",
    "categorical_features = ['HEAT', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'USECODE']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "onehot_encoder = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_features)], remainder='passthrough')\n",
    "data_encoded = onehot_encoder.fit_transform(data)\n",
    "\n",
    "# Update column names for one-hot encoded features\n",
    "encoded_feature_names = onehot_encoder.named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "data_encoded_df = pd.DataFrame(data_encoded, columns=list(encoded_feature_names) + list(data.columns.drop(categorical_features)))\n",
    "\n",
    "# Handle zero values in certain columns to prevent division errors in interaction features\n",
    "data_encoded_df['LANDAREA'].replace(0, np.nan, inplace=True)\n",
    "data_encoded_df['ROOMS'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "data_encoded_df['Rooms_Bathrooms'] = data_encoded_df['ROOMS'] * data_encoded_df['BATHRM']\n",
    "data_encoded_df['Building_Density'] = data_encoded_df['GBA'] / data_encoded_df['LANDAREA']\n",
    "data_encoded_df['Bedroom_Room_Ratio'] = data_encoded_df['BEDRM'] / data_encoded_df['ROOMS']\n",
    "\n",
    "# Fill NaN values after division\n",
    "data_encoded_df['Building_Density'].fillna(0, inplace=True)\n",
    "data_encoded_df['Bedroom_Room_Ratio'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert SALEDATE to datetime format and derive date-based features\n",
    "data_encoded_df['SALEDATE'] = pd.to_datetime(data_encoded_df['SALEDATE'])\n",
    "data_encoded_df['Property_Age'] = 2023 - data_encoded_df['AYB']\n",
    "data_encoded_df['Years_Since_Remodel'] = 2023 - data_encoded_df['YR_RMDL']\n",
    "data_encoded_df['Years_Between_Built_and_Remodel'] = data_encoded_df['YR_RMDL'] - data_encoded_df['AYB']\n",
    "data_encoded_df['Sale_Year'] = data_encoded_df['SALEDATE'].dt.year\n",
    "data_encoded_df['Sale_Month'] = data_encoded_df['SALEDATE'].dt.month\n",
    "\n",
    "# Define features for clustering (using numeric features only for simplicity)\n",
    "features_for_clustering = ['ROOMS', 'BATHRM', 'LANDAREA', 'GBA', 'PRICE']\n",
    "data_cluster = data_encoded_df[features_for_clustering].dropna()\n",
    "\n",
    "# Preserve original index for later merge\n",
    "data_cluster = data_cluster.reset_index()  # This adds the original index as a column\n",
    "\n",
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cluster[features_for_clustering])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "n_clusters = 5  # Set an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "data_cluster['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Merge cluster labels back to original data\n",
    "data_encoded_df = data_encoded_df.merge(data_cluster[['index', 'Cluster']], left_index=True, right_on='index', how='left')\n",
    "data_encoded_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Remove rows with NaN in Cluster column for SMOTE application\n",
    "data_with_clusters = data_encoded_df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Placeholder for augmented data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Apply random oversampling within each cluster\n",
    "for cluster in data_with_clusters['Cluster'].unique():\n",
    "    # Select data for the current cluster\n",
    "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "    \n",
    "    # Set the desired size for oversampling (e.g., double the original size of each cluster)\n",
    "    target_size = len(cluster_data) * 2\n",
    "    \n",
    "    # Perform random oversampling\n",
    "    cluster_augmented = resample(cluster_data, replace=True, n_samples=target_size, random_state=42)\n",
    "    \n",
    "    # Append to augmented data\n",
    "    augmented_data = pd.concat([augmented_data, cluster_augmented])\n",
    "\n",
    "# Combine original data with augmented data\n",
    "final_data = pd.concat([data_encoded_df, augmented_data], ignore_index=True)\n",
    "\n",
    "# Check final dataset shape and display a sample\n",
    "print(\"Original Data Shape (before encoding):\", data.shape)\n",
    "print(\"Data Encoded Shape:\", data_encoded_df.shape)\n",
    "print(\"Augmented Data Shape:\", augmented_data.shape)\n",
    "print(\"Final Data Shape:\", final_data.shape)\n",
    "final_data.head()\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after processing: 0\n",
      "Training set shape: (238678, 162)\n",
      "Test set shape: (59670, 162)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming final_data is the augmented dataset\n",
    "data = final_data.copy()\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Fill missing values in numeric columns with the median\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check that missing values have been handled\n",
    "print(\"Missing values after processing:\", data.isnull().sum().sum())\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "# Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Step 3: Detect and handle outliers\n",
    "# Use Z-score method to detect outliers; replace values with NaN if Z-score > 3\n",
    "z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "data[numeric_columns] = np.where(z_scores > 3, np.nan, data[numeric_columns])\n",
    "# Fill the NaN values resulting from outliers with the median\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "# Assuming 'PRICE' is the target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "\n",
    "# Ensure that only numeric columns are included in X for model training\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理有問題的數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types for Each Feature:\n",
      " HEAT_0.0                           float64\n",
      "HEAT_1.0                           float64\n",
      "HEAT_2.0                           float64\n",
      "HEAT_3.0                           float64\n",
      "HEAT_4.0                           float64\n",
      "                                    ...   \n",
      "Years_Since_Remodel                float64\n",
      "Years_Between_Built_and_Remodel    float64\n",
      "Sale_Year                          float64\n",
      "Sale_Month                         float64\n",
      "Cluster                            float64\n",
      "Length: 176, dtype: object\n",
      "\n",
      "Numeric Features Summary (including Q1, Q2, Q3, IQR):\n",
      "                                     count          mean           std  \\\n",
      "HEAT_0.0                         298348.0  2.781986e-04      0.016677   \n",
      "HEAT_1.0                         298348.0  3.596840e-01      0.479909   \n",
      "HEAT_2.0                         298348.0  7.206350e-04      0.026835   \n",
      "HEAT_3.0                         298348.0  1.783153e-03      0.042190   \n",
      "HEAT_4.0                         298348.0  6.133777e-04      0.024759   \n",
      "...                                   ...           ...           ...   \n",
      "Years_Since_Remodel              298348.0  1.099239e+01     16.823692   \n",
      "Years_Between_Built_and_Remodel  298348.0  3.573532e+02  73743.838172   \n",
      "Sale_Year                        298234.0  2.815105e-01      0.261889   \n",
      "Sale_Month                       298348.0 -3.975896e-15      1.000002   \n",
      "Cluster                          298348.0 -6.869375e-05      0.999876   \n",
      "\n",
      "                                      min        25%        50%        75%  \\\n",
      "HEAT_0.0                         0.000000   0.000000   0.000000   0.000000   \n",
      "HEAT_1.0                         0.000000   0.000000   0.000000   1.000000   \n",
      "HEAT_2.0                         0.000000   0.000000   0.000000   0.000000   \n",
      "HEAT_3.0                         0.000000   0.000000   0.000000   0.000000   \n",
      "HEAT_4.0                         0.000000   0.000000   0.000000   0.000000   \n",
      "...                                   ...        ...        ...        ...   \n",
      "Years_Since_Remodel              0.000000   4.000000   4.000000  14.000000   \n",
      "Years_Between_Built_and_Remodel  0.000000  80.000000  92.000000  92.000000   \n",
      "Sale_Year                       -1.064405   0.096186   0.354096   0.483050   \n",
      "Sale_Month                      -1.465284  -0.899848  -0.051692   0.796463   \n",
      "Cluster                         -1.017896  -1.017896   0.120695   0.120695   \n",
      "\n",
      "                                          max        IQR  \n",
      "HEAT_0.0                         1.000000e+00   0.000000  \n",
      "HEAT_1.0                         1.000000e+00   1.000000  \n",
      "HEAT_2.0                         1.000000e+00   0.000000  \n",
      "HEAT_3.0                         1.000000e+00   0.000000  \n",
      "HEAT_4.0                         1.000000e+00   0.000000  \n",
      "...                                       ...        ...  \n",
      "Years_Since_Remodel              2.003000e+03  10.000000  \n",
      "Years_Between_Built_and_Remodel  2.014007e+07  12.000000  \n",
      "Sale_Year                        6.120047e-01   0.386864  \n",
      "Sale_Month                       1.644618e+00   1.696310  \n",
      "Cluster                          2.397877e+00   1.138591  \n",
      "\n",
      "[163 rows x 9 columns]\n",
      "\n",
      "Percentage of Outliers in Each Numeric Feature:\n",
      "                                  Outliers (%)\n",
      "HEAT_0.0                             0.027820\n",
      "HEAT_1.0                             0.000000\n",
      "HEAT_2.0                             0.072063\n",
      "HEAT_3.0                             0.178315\n",
      "HEAT_4.0                             0.061338\n",
      "...                                       ...\n",
      "Years_Since_Remodel                  0.734042\n",
      "Years_Between_Built_and_Remodel      0.001341\n",
      "Sale_Year                            0.190119\n",
      "Sale_Month                           0.000000\n",
      "Cluster                              0.000000\n",
      "\n",
      "[163 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 檢查每個特徵的數據類型\n",
    "print(\"Data Types for Each Feature:\\n\", data.dtypes)\n",
    "\n",
    "# 計算每個數值特徵的描述統計量（包括 Q1, Q2 (中位數), Q3）\n",
    "numeric_summary = data.describe(percentiles=[0.25, 0.5, 0.75]).T  # 使用轉置方便閱讀\n",
    "numeric_summary['IQR'] = numeric_summary['75%'] - numeric_summary['25%']  # 計算 IQR\n",
    "\n",
    "# 顯示每個特徵的四分位數信息\n",
    "print(\"\\nNumeric Features Summary (including Q1, Q2, Q3, IQR):\\n\", numeric_summary)\n",
    "\n",
    "# 檢查極端值（異常值）百分比\n",
    "# 計算 Z-score 超過閾值的異常值比例\n",
    "from scipy import stats\n",
    "outliers_percentage = {}\n",
    "\n",
    "for col in data.select_dtypes(include=[np.number]).columns:\n",
    "    z_scores = stats.zscore(data[col].dropna())  # 排除缺失值，計算 Z-score\n",
    "    outliers_percentage[col] = (np.abs(z_scores) > 3).mean() * 100  # 計算 Z-score > 3 的異常值比例\n",
    "\n",
    "outliers_percentage_df = pd.DataFrame.from_dict(outliers_percentage, orient='index', columns=['Outliers (%)'])\n",
    "print(\"\\nPercentage of Outliers in Each Numeric Feature:\\n\", outliers_percentage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAT_0.0 unique values: [0. 1.]\n",
      "HEAT_1.0 unique values: [0. 1.]\n",
      "HEAT_2.0 unique values: [0. 1.]\n",
      "HEAT_3.0 unique values: [0. 1.]\n",
      "HEAT_4.0 unique values: [0. 1.]\n",
      "HEAT_5.0 unique values: [0. 1.]\n",
      "HEAT_6.0 unique values: [0. 1.]\n",
      "HEAT_7.0 unique values: [0. 1.]\n",
      "HEAT_8.0 unique values: [1. 0.]\n",
      "HEAT_9.0 unique values: [0. 1.]\n",
      "HEAT_10.0 unique values: [0. 1.]\n",
      "HEAT_11.0 unique values: [0. 1.]\n",
      "HEAT_12.0 unique values: [0. 1.]\n",
      "HEAT_13.0 unique values: [0. 1.]\n",
      "HEAT_nan unique values: [0. 1.]\n",
      "STYLE_0.0 unique values: [0. 1.]\n",
      "STYLE_1.0 unique values: [0. 1.]\n",
      "STYLE_2.0 unique values: [0. 1.]\n",
      "STYLE_3.0 unique values: [0. 1.]\n",
      "STYLE_4.0 unique values: [0. 1.]\n",
      "STYLE_5.0 unique values: [0. 1.]\n",
      "STYLE_6.0 unique values: [0. 1.]\n",
      "STYLE_7.0 unique values: [0. 1.]\n",
      "STYLE_8.0 unique values: [0. 1.]\n",
      "STYLE_9.0 unique values: [0. 1.]\n",
      "STYLE_10.0 unique values: [1. 0.]\n",
      "STYLE_11.0 unique values: [0. 1.]\n",
      "STYLE_12.0 unique values: [0. 1.]\n",
      "STYLE_13.0 unique values: [0. 1.]\n",
      "STYLE_14.0 unique values: [0. 1.]\n",
      "STYLE_15.0 unique values: [0. 1.]\n",
      "STYLE_94.0 unique values: [0. 1.]\n",
      "STYLE_99.0 unique values: [0. 1.]\n",
      "STYLE_nan unique values: [0. 1.]\n",
      "STRUCT_0.0 unique values: [0. 1.]\n",
      "STRUCT_1.0 unique values: [0. 1.]\n",
      "STRUCT_2.0 unique values: [0. 1.]\n",
      "STRUCT_4.0 unique values: [0. 1.]\n",
      "STRUCT_5.0 unique values: [0. 1.]\n",
      "STRUCT_6.0 unique values: [1. 0.]\n",
      "STRUCT_7.0 unique values: [0. 1.]\n",
      "STRUCT_8.0 unique values: [0. 1.]\n",
      "STRUCT_13.0 unique values: [0. 1.]\n",
      "STRUCT_nan unique values: [0. 1.]\n",
      "HEAT_D unique values: ['Ht Pump' 'Forced Air' 'Warm Cool' 'Hot Water Rad' 'Water Base Brd'\n",
      " 'Elec Base Brd' 'Wall Furnace' 'Electric Rad' 'No Data' 'Air Exchng'\n",
      " 'Evp Cool' 'Gravity Furnac' 'Ind Unit' 'Air-Oil']\n",
      "STYLE_D unique values: ['4 Story' '3 Story' '2.5 Story Fin' '1.5 Story Fin' '2 Story'\n",
      " '3.5 Story Fin' '3.5 Story Unfin' '2.5 Story Unfin' '4.5 Story Fin'\n",
      " '1 Story' 'No Data' '4.5 Story Unfin' '1.5 Story Unfin' 'Bi-Level'\n",
      " 'Split Level' 'Split Foyer' 'Vacant' 'Outbuildings']\n",
      "STRUCT_D unique values: ['Row End' 'Row Inside' 'Single' 'Multi' 'Semi-Detached' 'Town Inside'\n",
      " 'Town End' 'No Data' 'Vacant Land']\n"
     ]
    }
   ],
   "source": [
    "# 檢查 One-Hot Encoding 特徵的唯一值\n",
    "one_hot_columns = [col for col in data.columns if 'HEAT_' in col or 'STYLE_' in col or 'STRUCT_' in col]  # 替換為您使用 One-Hot Encoding 的特徵前綴\n",
    "for col in one_hot_columns:\n",
    "    print(f\"{col} unique values:\", data[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years_Since_Remodel'] = data['Years_Since_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "data['Years_Between_Built_and_Remodel'] = data['Years_Between_Built_and_Remodel'].apply(lambda x: x if x >= 0 else np.nan)\n",
    "\n",
    "# 用中位數填補 NaN\n",
    "data['Years_Since_Remodel'].fillna(data['Years_Since_Remodel'].median(), inplace=True)\n",
    "data['Years_Between_Built_and_Remodel'].fillna(data['Years_Between_Built_and_Remodel'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Sale_Month: [ 0.51374454  0.23102613  1.07918136 -0.6171291   1.64461818 -0.05169228\n",
      " -0.33441069 -1.18256592 -1.46528434 -0.89984751  0.79646295  1.36189977]\n",
      "Unique values in Cluster: [ 1.25928618  2.39787743 -1.01789634  0.12069492]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in Sale_Month:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Sale_Month after processing: [ 8  7 10  4 12  6  5  2  1  3  9 11]\n",
      "Unique values in Cluster after processing: [2. 3. 0. 1. 4.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 恢復原始數據\n",
    "data = final_data.copy()  # 使用未經標準化的原始數據進行重新處理\n",
    "\n",
    "# 填充缺失值\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# 確認不需要標準化的特徵（例如 Sale_Month 和 Cluster）\n",
    "exclude_columns = ['Sale_Month', 'Cluster']\n",
    "numeric_columns_for_scaling = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "# 對需要標準化的數值特徵進行標準化\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns_for_scaling] = scaler.fit_transform(data[numeric_columns_for_scaling])\n",
    "\n",
    "# 確認 Sale_Month 和 Cluster 列保持原始值\n",
    "print(\"Unique values in Sale_Month after processing:\", data['Sale_Month'].unique())\n",
    "print(\"Unique values in Cluster after processing:\", data['Cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Cluster after conversion: [2 3 0 1 4]\n"
     ]
    }
   ],
   "source": [
    "# 將 Cluster 列轉換為整數型\n",
    "data['Cluster'] = data['Cluster'].astype(int)\n",
    "\n",
    "# 檢查轉換後的值\n",
    "print(\"Unique values in Cluster after conversion:\", data['Cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除極端值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after adjusting IQR threshold: (194710, 176)\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers_iqr(df, columns, iqr_multiplier=1.5):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "        df[column] = np.where((df[column] < lower_bound) | (df[column] > upper_bound), np.nan, df[column])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用更大的 IQR 邊界（例如 2 或 2.5）去除極端值\n",
    "data_cleaned = remove_outliers_iqr(data, numeric_columns, iqr_multiplier=1.5)\n",
    "print(\"Data shape after adjusting IQR threshold:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after removing outliers with Z-score: (194710, 176)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers_zscore(df, columns, z_threshold=3):\n",
    "    for column in columns:\n",
    "        # 計算 Z-score，並保持原始數據的形狀\n",
    "        z_scores = np.abs(stats.zscore(df[column].fillna(df[column].median())))  # 填充缺失值以便計算 Z-score\n",
    "        # 將 Z-score 超過閾值的數值標記為 NaN\n",
    "        df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n",
    "    # 刪除包含 NaN 的行\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# 使用 Z-score 方法去除異常值\n",
    "data_cleaned = remove_outliers_zscore(data, numeric_columns, z_threshold=3)\n",
    "print(\"Data shape after removing outliers with Z-score:\", data_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r3/pnglxfwx2yv5b34nx9j2ysgh0000gn/T/ipykernel_99740/1221692749.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = np.where(z_scores > z_threshold, np.nan, df[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after stricter outlier removal for key features: (183898, 176)\n"
     ]
    }
   ],
   "source": [
    "# 針對特定特徵使用更嚴格的異常值閾值\n",
    "important_columns = ['PRICE', 'ROOMS', 'GBA']  # 替換為您認為重要的特徵\n",
    "data_important_cleaned = remove_outliers_iqr(data, important_columns, iqr_multiplier=1.5)\n",
    "data_important_cleaned = remove_outliers_zscore(data_important_cleaned, important_columns, z_threshold=2.5)\n",
    "print(\"Data shape after stricter outlier removal for key features:\", data_important_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after quantile clipping: (298348, 176)\n"
     ]
    }
   ],
   "source": [
    "def quantile_clipping(df, columns, lower_quantile=0.025, upper_quantile=0.975):\n",
    "    for column in columns:\n",
    "        lower_bound = df[column].quantile(lower_quantile)\n",
    "        upper_bound = df[column].quantile(upper_quantile)\n",
    "        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "# 使用分位數截斷法去除異常值\n",
    "data_clipped = quantile_clipping(data, numeric_columns)\n",
    "print(\"Data shape after quantile clipping:\", data_clipped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將數據分割為訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (298348, 176)\n",
      "PRICE column found with shape: (298348,)\n",
      "X shape before splitting: (298348, 175)\n",
      "y shape before splitting: (298348,)\n",
      "Training set shape: (238678, 175)\n",
      "Test set shape: (59670, 175)\n"
     ]
    }
   ],
   "source": [
    "# 檢查 data 是否有數據\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# 確認 'PRICE' 列是否存在\n",
    "if 'PRICE' not in data.columns:\n",
    "    print(\"'PRICE' column not found in data.\")\n",
    "else:\n",
    "    print(\"PRICE column found with shape:\", data['PRICE'].shape)\n",
    "\n",
    "# 確認 X 和 y 的形狀\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "print(\"X shape before splitting:\", X.shape)\n",
    "print(\"y shape before splitting:\", y.shape)\n",
    "\n",
    "# 分割數據集（在確認 X 和 y 有數據之後再執行）\n",
    "if X.shape[0] > 0 and y.shape[0] > 0:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "else:\n",
    "    print(\"X or y has no data, please check your preprocessing steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in each column of X_train after conversion:\n",
      " SSL                   238677\n",
      "HEAT_D                238678\n",
      "AC                    238615\n",
      "ROOMS                  15954\n",
      "QUALIFIED             238678\n",
      "GBA                    21308\n",
      "STYLE_D               238678\n",
      "STRUCT_D              238678\n",
      "GRADE_D               238678\n",
      "CNDTN_D               238678\n",
      "EXTWALL_D             238678\n",
      "ROOF_D                238678\n",
      "INTWALL_D             238678\n",
      "LANDAREA               28243\n",
      "GIS_LAST_MOD_DTTM     238678\n",
      "Building_Density        7691\n",
      "Bedroom_Room_Ratio     22164\n",
      "Sale_Year              18503\n",
      "Cluster                25873\n",
      "dtype: int64\n",
      "Number of NaNs in each column of X_test after conversion:\n",
      " SSL                   59670\n",
      "HEAT_D                59670\n",
      "AC                    59654\n",
      "ROOMS                  3985\n",
      "QUALIFIED             59670\n",
      "GBA                    5331\n",
      "STYLE_D               59670\n",
      "STRUCT_D              59670\n",
      "GRADE_D               59670\n",
      "CNDTN_D               59670\n",
      "EXTWALL_D             59670\n",
      "ROOF_D                59670\n",
      "INTWALL_D             59670\n",
      "LANDAREA               7020\n",
      "GIS_LAST_MOD_DTTM     59670\n",
      "Building_Density       1923\n",
      "Bedroom_Room_Ratio     5654\n",
      "Sale_Year              4589\n",
      "Cluster                6422\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 將所有特徵列轉換為數值類型，無法轉換的值設為 NaN\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "\n",
    "# 檢查轉換後是否還有 NaN 值\n",
    "nan_counts_train = X_train.isnull().sum()\n",
    "nan_counts_test = X_test.isnull().sum()\n",
    "\n",
    "print(\"Number of NaNs in each column of X_train after conversion:\\n\", nan_counts_train[nan_counts_train > 0])\n",
    "print(\"Number of NaNs in each column of X_test after conversion:\\n\", nan_counts_test[nan_counts_test > 0])\n",
    "\n",
    "# 用中位數填補 NaN 值（或根據需要選擇其他填補策略）\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除含有大量 NaN 值的欄位（如果不必要）\n",
    "columns_to_drop = ['SSL', 'HEAT_D', 'AC', 'STYLE_D', 'STRUCT_D', 'GRADE_D', 'CNDTN_D', 'EXTWALL_D', 'ROOF_D', \n",
    "                   'INTWALL_D', 'GIS_LAST_MOD_DTTM', 'Sale_Year', 'Cluster',  'QUALIFIED']  # 根據需要調整列表\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n",
    "X_test = X_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in each column of X_train after filling:\n",
      " 0\n",
      "Number of NaNs in each column of X_test after filling:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# 確認沒有 NaN 值\n",
    "print(\"Number of NaNs in each column of X_train after filling:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in each column of X_test after filling:\\n\", X_test.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaN values in each column of X_train:\n",
      " HEAT_0.0        0\n",
      "EXTWALL_15.0    0\n",
      "ROOF_11.0       0\n",
      "ROOF_12.0       0\n",
      "ROOF_13.0       0\n",
      "ROOF_14.0       0\n",
      "ROOF_15.0       0\n",
      "ROOF_nan        0\n",
      "INTWALL_0.0     0\n",
      "INTWALL_1.0     0\n",
      "INTWALL_2.0     0\n",
      "INTWALL_3.0     0\n",
      "INTWALL_4.0     0\n",
      "INTWALL_5.0     0\n",
      "INTWALL_6.0     0\n",
      "INTWALL_7.0     0\n",
      "INTWALL_8.0     0\n",
      "INTWALL_9.0     0\n",
      "INTWALL_10.0    0\n",
      "ROOF_10.0       0\n",
      "dtype: int64\n",
      "Remaining NaN values in each column of X_test:\n",
      " HEAT_0.0        0\n",
      "EXTWALL_15.0    0\n",
      "ROOF_11.0       0\n",
      "ROOF_12.0       0\n",
      "ROOF_13.0       0\n",
      "ROOF_14.0       0\n",
      "ROOF_15.0       0\n",
      "ROOF_nan        0\n",
      "INTWALL_0.0     0\n",
      "INTWALL_1.0     0\n",
      "INTWALL_2.0     0\n",
      "INTWALL_3.0     0\n",
      "INTWALL_4.0     0\n",
      "INTWALL_5.0     0\n",
      "INTWALL_6.0     0\n",
      "INTWALL_7.0     0\n",
      "INTWALL_8.0     0\n",
      "INTWALL_9.0     0\n",
      "INTWALL_10.0    0\n",
      "ROOF_10.0       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 查看每個欄位中剩餘的 NaN 數量\n",
    "print(\"Remaining NaN values in each column of X_train:\\n\", X_train.isnull().sum().sort_values(ascending=False).head(20))\n",
    "print(\"Remaining NaN values in each column of X_test:\\n\", X_test.isnull().sum().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaN values in X_train? HEAT_0.0                           False\n",
      "HEAT_1.0                           False\n",
      "HEAT_2.0                           False\n",
      "HEAT_3.0                           False\n",
      "HEAT_4.0                           False\n",
      "                                   ...  \n",
      "Bedroom_Room_Ratio                 False\n",
      "Property_Age                       False\n",
      "Years_Since_Remodel                False\n",
      "Years_Between_Built_and_Remodel    False\n",
      "Sale_Month                         False\n",
      "Length: 161, dtype: bool\n",
      "Are there any NaN values in X_test? HEAT_0.0                           False\n",
      "HEAT_1.0                           False\n",
      "HEAT_2.0                           False\n",
      "HEAT_3.0                           False\n",
      "HEAT_4.0                           False\n",
      "                                   ...  \n",
      "Bedroom_Room_Ratio                 False\n",
      "Property_Age                       False\n",
      "Years_Since_Remodel                False\n",
      "Years_Between_Built_and_Remodel    False\n",
      "Sale_Month                         False\n",
      "Length: 161, dtype: bool\n",
      "Are there any Infinity values in X_train? HEAT_0.0                           False\n",
      "HEAT_1.0                           False\n",
      "HEAT_2.0                           False\n",
      "HEAT_3.0                           False\n",
      "HEAT_4.0                           False\n",
      "                                   ...  \n",
      "Bedroom_Room_Ratio                 False\n",
      "Property_Age                       False\n",
      "Years_Since_Remodel                False\n",
      "Years_Between_Built_and_Remodel    False\n",
      "Sale_Month                         False\n",
      "Length: 161, dtype: bool\n",
      "Are there any Infinity values in X_test? HEAT_0.0                           False\n",
      "HEAT_1.0                           False\n",
      "HEAT_2.0                           False\n",
      "HEAT_3.0                           False\n",
      "HEAT_4.0                           False\n",
      "                                   ...  \n",
      "Bedroom_Room_Ratio                 False\n",
      "Property_Age                       False\n",
      "Years_Since_Remodel                False\n",
      "Years_Between_Built_and_Remodel    False\n",
      "Sale_Month                         False\n",
      "Length: 161, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Are there any NaN values in X_train?\", np.isnan(X_train).any())\n",
    "print(\"Are there any NaN values in X_test?\", np.isnan(X_test).any())\n",
    "print(\"Are there any Infinity values in X_train?\", np.isinf(X_train).any())\n",
    "print(\"Are there any Infinity values in X_test?\", np.isinf(X_test).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有的 Infinity 值替換為 NaN\n",
    "X_train = np.where(np.isinf(X_train), np.nan, X_train)\n",
    "X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "\n",
    "# 用中位數替換 NaN 值\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in X_train: 1.7276688e+18\n",
      "Minimum value in X_train: -2.2089708e+18\n",
      "Maximum value in X_test: 1.7276688e+18\n",
      "Minimum value in X_test: -2.2089708e+18\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum value in X_train:\", np.nanmax(X_train))\n",
    "print(\"Minimum value in X_train:\", np.nanmin(X_train))\n",
    "print(\"Maximum value in X_test:\", np.nanmax(X_test))\n",
    "print(\"Minimum value in X_test:\", np.nanmin(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數值限制在一個合理的範圍\n",
    "X_train = np.clip(X_train, -1e6, 1e6)\n",
    "X_test = np.clip(X_test, -1e6, 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 逐列檢查 NaN 和 Infinity\n",
    "for column in range(X_train.shape[1]):\n",
    "    if np.isnan(X_train[:, column]).any():\n",
    "        print(f\"NaN found in column {column} of X_train\")\n",
    "    if np.isinf(X_train[:, column]).any():\n",
    "        print(f\"Infinity found in column {column} of X_train\")\n",
    "\n",
    "for column in range(X_test.shape[1]):\n",
    "    if np.isnan(X_test[:, column]).any():\n",
    "        print(f\"NaN found in column {column} of X_test\")\n",
    "    if np.isinf(X_test[:, column]).any():\n",
    "        print(f\"Infinity found in column {column} of X_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in X_train: 1000000.0\n",
      "Minimum value in X_train: -1000000.0\n",
      "Maximum value in X_test: 1000000.0\n",
      "Minimum value in X_test: -1000000.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum value in X_train:\", np.max(X_train))\n",
    "print(\"Minimum value in X_train:\", np.min(X_train))\n",
    "print(\"Maximum value in X_test:\", np.max(X_test))\n",
    "print(\"Minimum value in X_test:\", np.min(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in X_train:\n",
      " 0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "        ...   \n",
      "156    float64\n",
      "157    float64\n",
      "158    float64\n",
      "159    float64\n",
      "160    float64\n",
      "Length: 161, dtype: object\n",
      "Data types in X_test:\n",
      " 0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "        ...   \n",
      "156    float64\n",
      "157    float64\n",
      "158    float64\n",
      "159    float64\n",
      "160    float64\n",
      "Length: 161, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 將 X_train 和 X_test 轉回 DataFrame 格式\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# 檢查數據類型\n",
    "print(\"Data types in X_train:\\n\", X_train.dtypes)\n",
    "print(\"Data types in X_test:\\n\", X_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認每一列中是否含有非數值的字符\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].apply(lambda x: isinstance(x, str)).any():\n",
    "        print(f\"Column {col} contains non-numeric values in X_train\")\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].apply(lambda x: isinstance(x, str)).any():\n",
    "        print(f\"Column {col} contains non-numeric values in X_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 強制轉換為 float64，無法轉換的設為 NaN\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 再次用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pattysung/DC-Housing-Price-Prediction/Washington, D.C. Housing Price Prediction_V2.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 訓練線性回歸模型\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lr_model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#Y115sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m lr_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#Y115sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 預測並評估\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred_lr \u001b[39m=\u001b[39m lr_model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:662\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    660\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 662\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    663\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    664\u001b[0m )\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:979\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[0;32m--> 979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric)\n\u001b[1;32m    981\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:989\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m multi_output:\n\u001b[0;32m--> 989\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    990\u001b[0m         y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 訓練線性回歸模型\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測並評估\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_lr)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_lr)\n",
    "print(\"R-squared (R²):\", r2_lr)\n",
    "\n",
    "# 訓練隨機森林模型\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測並評估\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_rf)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_rf)\n",
    "print(\"R-squared (R²):\", r2_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: 將 Infinity 值替換為 NaN（適用於 numpy.ndarray）\n",
    "X_train[np.isinf(X_train)] = np.nan\n",
    "X_test[np.isinf(X_test)] = np.nan\n",
    "\n",
    "# Step 2: 使用中位數填補 NaN 值\n",
    "# 因為是 numpy.ndarray 格式，需計算中位數後再進行填補\n",
    "# 計算每列的中位數，並將 NaN 值替換為中位數\n",
    "col_medians_train = np.nanmedian(X_train, axis=0)\n",
    "col_medians_test = np.nanmedian(X_test, axis=0)\n",
    "\n",
    "# 使用中位數填補 NaN 值\n",
    "inds_train = np.where(np.isnan(X_train))\n",
    "inds_test = np.where(np.isnan(X_test))\n",
    "\n",
    "X_train[inds_train] = np.take(col_medians_train, inds_train[1])\n",
    "X_test[inds_test] = np.take(col_medians_test, inds_test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再度確認有沒有空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaN values in X_train? False\n",
      "Are there any NaN values in X_test? False\n",
      "Are there any Infinity values in X_train? False\n",
      "Are there any Infinity values in X_test? False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 檢查 NaN 和 Infinity 值\n",
    "print(\"Are there any NaN values in X_train?\", np.isnan(X_train).any())\n",
    "print(\"Are there any NaN values in X_test?\", np.isnan(X_test).any())\n",
    "print(\"Are there any Infinity values in X_train?\", np.isinf(X_train).any())\n",
    "print(\"Are there any Infinity values in X_test?\", np.isinf(X_test).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in X_train: 488.5458013328945\n",
      "Minimum value in X_train: -282.0607454559197\n",
      "Maximum value in X_test: 488.5458013328945\n",
      "Minimum value in X_test: -282.0607454559197\n"
     ]
    }
   ],
   "source": [
    "# 檢查 X_train 和 X_test 中的最大和最小值\n",
    "print(\"Maximum value in X_train:\", np.nanmax(X_train))\n",
    "print(\"Minimum value in X_train:\", np.nanmin(X_train))\n",
    "print(\"Maximum value in X_test:\", np.nanmax(X_test))\n",
    "print(\"Minimum value in X_test:\", np.nanmin(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types for Each Feature in X:\n",
      " HEAT_0.0                           float64\n",
      "HEAT_1.0                           float64\n",
      "HEAT_2.0                           float64\n",
      "HEAT_3.0                           float64\n",
      "HEAT_4.0                           float64\n",
      "                                    ...   \n",
      "Years_Since_Remodel                float64\n",
      "Years_Between_Built_and_Remodel    float64\n",
      "Sale_Year                          float64\n",
      "Sale_Month                         float64\n",
      "Cluster                            float64\n",
      "Length: 162, dtype: object\n",
      "Non-numeric columns: []\n"
     ]
    }
   ],
   "source": [
    "# 檢查所有特徵的數據類型\n",
    "print(\"Data Types for Each Feature in X:\\n\", X.dtypes)\n",
    "\n",
    "# 找出非數值列\n",
    "non_numeric_columns = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除無法轉換為數值的列（如不必要的字符串列）\n",
    "X = X.drop(columns=non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有列強制轉換為數值類型，無法轉換的值設為 NaN\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 再次檢查是否有 NaN 並填補\n",
    "X = X.fillna(X.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in X_train before standardization:\n",
      " 0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "        ...   \n",
      "157    float64\n",
      "158    float64\n",
      "159    float64\n",
      "160    float64\n",
      "161    float64\n",
      "Length: 162, dtype: object\n",
      "Data types in X_test before standardization:\n",
      " 0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "        ...   \n",
      "157    float64\n",
      "158    float64\n",
      "159    float64\n",
      "160    float64\n",
      "161    float64\n",
      "Length: 162, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 確認標準化之前所有列的數據類型\n",
    "print(\"Data types in X_train before standardization:\\n\", pd.DataFrame(X_train).dtypes)\n",
    "print(\"Data types in X_test before standardization:\\n\", pd.DataFrame(X_test).dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假設您已經有原始的列名\n",
    "X_train = pd.DataFrame(X_train, columns= X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns= X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in X_train after conversion:\n",
      " 0\n",
      "Number of NaNs in X_test after conversion:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# 強制將所有列轉換為數值類型，無法轉換的值設置為 NaN\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 檢查是否有 NaN 值\n",
    "print(\"Number of NaNs in X_train after conversion:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in X_test after conversion:\\n\", X_test.isnull().sum().sum())\n",
    "\n",
    "# 使用中位數填補 NaN 值\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in X_train after conversion:\n",
      " HEAT_0.0                           float64\n",
      "HEAT_1.0                           float64\n",
      "HEAT_2.0                           float64\n",
      "HEAT_3.0                           float64\n",
      "HEAT_4.0                           float64\n",
      "                                    ...   \n",
      "Years_Since_Remodel                float64\n",
      "Years_Between_Built_and_Remodel    float64\n",
      "Sale_Year                          float64\n",
      "Sale_Month                         float64\n",
      "Cluster                            float64\n",
      "Length: 162, dtype: object\n",
      "Data types in X_test after conversion:\n",
      " HEAT_0.0                           float64\n",
      "HEAT_1.0                           float64\n",
      "HEAT_2.0                           float64\n",
      "HEAT_3.0                           float64\n",
      "HEAT_4.0                           float64\n",
      "                                    ...   \n",
      "Years_Since_Remodel                float64\n",
      "Years_Between_Built_and_Remodel    float64\n",
      "Sale_Year                          float64\n",
      "Sale_Month                         float64\n",
      "Cluster                            float64\n",
      "Length: 162, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 確認所有列的數據類型\n",
    "print(\"Data types in X_train after conversion:\\n\", X_train.dtypes)\n",
    "print(\"Data types in X_test after conversion:\\n\", X_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    non_numeric_count = X_train[col].apply(lambda x: isinstance(x, (str, bytes))).sum()\n",
    "    if non_numeric_count > 0:\n",
    "        print(f\"Column {col} has {non_numeric_count} non-numeric values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in X_train after conversion:\n",
      " 0\n",
      "Number of NaNs in X_test after conversion:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in X_train after conversion:\\n\", X_train.isnull().sum().sum())\n",
    "print(\"Number of NaNs in X_test after conversion:\\n\", X_test.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 標準化數據\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pattysung/DC-Housing-Price-Prediction/Washington, D.C. Housing Price Prediction_V2.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr_model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 訓練模型\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lr_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 預測\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pattysung/DC-Housing-Price-Prediction/Washington%2C%20D.C.%20Housing%20Price%20Prediction_V2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_pred_lr \u001b[39m=\u001b[39m lr_model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:662\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    660\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 662\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    663\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    664\u001b[0m )\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:979\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[0;32m--> 979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric)\n\u001b[1;32m    981\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:989\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m multi_output:\n\u001b[0;32m--> 989\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    990\u001b[0m         y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# 初始化線性回歸模型\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# 訓練模型\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# 評估模型表現\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_lr)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_lr)\n",
    "print(\"R-squared (R²):\", r2_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Performance:\n",
      "Mean Squared Error (MSE): 37321710498.20493\n",
      "R-squared (R²): 0.9272339222749278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 初始化並訓練隨機森林模型\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 計算隨機森林模型的評估指標\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_rf)\n",
    "print(\"R-squared (R²):\", r2_rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "34448d0e5552d4eabed10596197defac81e617a6f3d1876b5ea6fe118997847f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
