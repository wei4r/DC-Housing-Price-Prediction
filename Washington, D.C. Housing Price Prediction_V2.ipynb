{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (before encoding): (109034, 39)\n",
      "Data Encoded Shape: (109034, 176)\n",
      "Augmented Data Shape: (189314, 176)\n",
      "Final Data Shape: (298348, 176)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>LANDAREA</th>\n",
       "      <th>Building_Density</th>\n",
       "      <th>Bedroom_Room_Ratio</th>\n",
       "      <th>Sale_Year</th>\n",
       "      <th>Sale_Month</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>296126.000000</td>\n",
       "      <td>298310.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>298348.000000</td>\n",
       "      <td>283971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.498886</td>\n",
       "      <td>3356.172287</td>\n",
       "      <td>0.742864</td>\n",
       "      <td>0.468994</td>\n",
       "      <td>2005.016424</td>\n",
       "      <td>6.182840</td>\n",
       "      <td>0.888629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.333140</td>\n",
       "      <td>5673.380941</td>\n",
       "      <td>0.459735</td>\n",
       "      <td>0.134558</td>\n",
       "      <td>31.018729</td>\n",
       "      <td>3.537094</td>\n",
       "      <td>0.899906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>0.397614</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2313.000000</td>\n",
       "      <td>0.635209</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>4140.000000</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>942632.000000</td>\n",
       "      <td>4.988943</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ROOMS       LANDAREA  Building_Density  Bedroom_Room_Ratio  \\\n",
       "count  296126.000000  298310.000000     298348.000000       298348.000000   \n",
       "mean        7.498886    3356.172287          0.742864            0.468994   \n",
       "std         2.333140    5673.380941          0.459735            0.134558   \n",
       "min         1.000000       1.000000          0.000000            0.000000   \n",
       "25%         6.000000    1571.000000          0.397614            0.400000   \n",
       "50%         7.000000    2313.000000          0.635209            0.500000   \n",
       "75%         8.000000    4140.000000          0.988235            0.500000   \n",
       "max        48.000000  942632.000000          4.988943            8.000000   \n",
       "\n",
       "           Sale_Year     Sale_Month        Cluster  \n",
       "count  298348.000000  298348.000000  283971.000000  \n",
       "mean     2005.016424       6.182840       0.888629  \n",
       "std        31.018729       3.537094       0.899906  \n",
       "min      1900.000000       1.000000       0.000000  \n",
       "25%      2005.000000       3.000000       0.000000  \n",
       "50%      2015.000000       6.000000       1.000000  \n",
       "75%      2020.000000       9.000000       1.000000  \n",
       "max      2024.000000      12.000000       4.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Computer_Assisted_Mass_Appraisal_-_Residential.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify categorical columns for One-Hot Encoding\n",
    "categorical_features = ['HEAT', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'USECODE']\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "onehot_encoder = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_features)], remainder='passthrough')\n",
    "data_encoded = onehot_encoder.fit_transform(data)\n",
    "\n",
    "# Update column names for one-hot encoded features\n",
    "encoded_feature_names = onehot_encoder.named_transformers_['onehot'].get_feature_names_out(categorical_features)\n",
    "data_encoded_df = pd.DataFrame(data_encoded, columns=list(encoded_feature_names) + list(data.columns.drop(categorical_features)))\n",
    "\n",
    "# Handle zero values in certain columns to prevent division errors in interaction features\n",
    "data_encoded_df['LANDAREA'].replace(0, np.nan, inplace=True)\n",
    "data_encoded_df['ROOMS'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "data_encoded_df['Rooms_Bathrooms'] = data_encoded_df['ROOMS'] * data_encoded_df['BATHRM']\n",
    "data_encoded_df['Building_Density'] = data_encoded_df['GBA'] / data_encoded_df['LANDAREA']\n",
    "data_encoded_df['Bedroom_Room_Ratio'] = data_encoded_df['BEDRM'] / data_encoded_df['ROOMS']\n",
    "\n",
    "# Fill NaN values after division\n",
    "data_encoded_df['Building_Density'].fillna(0, inplace=True)\n",
    "data_encoded_df['Bedroom_Room_Ratio'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert SALEDATE to datetime format and derive date-based features\n",
    "data_encoded_df['SALEDATE'] = pd.to_datetime(data_encoded_df['SALEDATE'])\n",
    "data_encoded_df['Property_Age'] = 2023 - data_encoded_df['AYB']\n",
    "data_encoded_df['Years_Since_Remodel'] = 2023 - data_encoded_df['YR_RMDL']\n",
    "data_encoded_df['Years_Between_Built_and_Remodel'] = data_encoded_df['YR_RMDL'] - data_encoded_df['AYB']\n",
    "data_encoded_df['Sale_Year'] = data_encoded_df['SALEDATE'].dt.year\n",
    "data_encoded_df['Sale_Month'] = data_encoded_df['SALEDATE'].dt.month\n",
    "\n",
    "# Define features for clustering (using numeric features only for simplicity)\n",
    "features_for_clustering = ['ROOMS', 'BATHRM', 'LANDAREA', 'GBA', 'PRICE']\n",
    "data_cluster = data_encoded_df[features_for_clustering].dropna()\n",
    "\n",
    "# Preserve original index for later merge\n",
    "data_cluster = data_cluster.reset_index()  # This adds the original index as a column\n",
    "\n",
    "# Normalize features for clustering\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cluster[features_for_clustering])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "n_clusters = 5  # Set an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "data_cluster['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Merge cluster labels back to original data\n",
    "data_encoded_df = data_encoded_df.merge(data_cluster[['index', 'Cluster']], left_index=True, right_on='index', how='left')\n",
    "data_encoded_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Remove rows with NaN in Cluster column for SMOTE application\n",
    "data_with_clusters = data_encoded_df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Placeholder for augmented data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Apply random oversampling within each cluster\n",
    "for cluster in data_with_clusters['Cluster'].unique():\n",
    "    # Select data for the current cluster\n",
    "    cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "    \n",
    "    # Set the desired size for oversampling (e.g., double the original size of each cluster)\n",
    "    target_size = len(cluster_data) * 2\n",
    "    \n",
    "    # Perform random oversampling\n",
    "    cluster_augmented = resample(cluster_data, replace=True, n_samples=target_size, random_state=42)\n",
    "    \n",
    "    # Append to augmented data\n",
    "    augmented_data = pd.concat([augmented_data, cluster_augmented])\n",
    "\n",
    "# Combine original data with augmented data\n",
    "final_data = pd.concat([data_encoded_df, augmented_data], ignore_index=True)\n",
    "\n",
    "# Check final dataset shape and display a sample\n",
    "print(\"Original Data Shape (before encoding):\", data.shape)\n",
    "print(\"Data Encoded Shape:\", data_encoded_df.shape)\n",
    "print(\"Augmented Data Shape:\", augmented_data.shape)\n",
    "print(\"Final Data Shape:\", final_data.shape)\n",
    "final_data.head()\n",
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after processing: 0\n",
      "Training set shape: (238678, 162)\n",
      "Test set shape: (59670, 162)\n",
      "Original feature count: 162\n",
      "Reduced feature count with PCA: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming final_data is the augmented dataset\n",
    "data = final_data.copy()\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Fill missing values in numeric columns with the median\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check that missing values have been handled\n",
    "print(\"Missing values after processing:\", data.isnull().sum().sum())\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "# Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Step 3: Detect and handle outliers\n",
    "# Use Z-score method to detect outliers; replace values with NaN if Z-score > 3\n",
    "z_scores = np.abs(stats.zscore(data[numeric_columns]))\n",
    "data[numeric_columns] = np.where(z_scores > 3, np.nan, data[numeric_columns])\n",
    "# Fill the NaN values resulting from outliers with the median\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "# Assuming 'PRICE' is the target variable\n",
    "X = data.drop(columns=['PRICE'])\n",
    "y = data['PRICE']\n",
    "\n",
    "# Ensure that only numeric columns are included in X for PCA\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Optional Step 5: Feature selection using PCA\n",
    "# Use PCA to reduce features while retaining 95% of the variance\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Original feature count:\", X_train.shape[1])\n",
    "print(\"Reduced feature count with PCA:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
